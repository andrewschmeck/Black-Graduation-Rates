{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the needed modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Gr_Lakes_public_financials.csv\").iloc[:, 1:]\n",
    "df.set_index(['year','UNITID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "# df = X.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "# df['pairs'] = list(zip(df.level_0, df.level_1))\n",
    "# df.set_index(['pairs'], inplace = True)\n",
    "# df.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "# df.columns = ['cc']\n",
    "# df.drop_duplicates(inplace=True)\n",
    "# df[(df.cc>.75) & (df.cc <1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X and y dataframes and train-test split them\n",
    "y = df['GBA6RTBK']\n",
    "X = df.drop(columns = ['GBA6RTBK'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F1M07     726\n",
       "F1M08     726\n",
       "F1M06     726\n",
       "F1M05     726\n",
       "F1A19     522\n",
       "         ... \n",
       "F1C012     12\n",
       "F1C021     12\n",
       "F1C022     12\n",
       "F1C031     12\n",
       "F1E06      12\n",
       "Length: 211, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Concatenate to one dataframe, check for nan's\n",
    "df = pd.concat([X_train, y_train], axis=1)\n",
    "df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(780, 211)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns with 40% missing values\n",
    "res2 = df.columns[df.isnull().sum() > 311]\n",
    "df.drop(res2, inplace=True, axis=1)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split back\n",
    "y_train = df['GBA6RTBK']\n",
    "X_train = df.drop(columns = ['GBA6RTBK'], axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bin target to binary\n",
    "y_train.loc[y_train > .604] = 1\n",
    "y_train.loc[y_train < .604] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[['F1B19','F1STSVPC','F1TUFEFT','F1E09','F1A14','F1OTEXFT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up pipeline for scaling continuous variables\n",
    "continuous_pipeline = Pipeline(steps=[\n",
    "    ('ss', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = ColumnTransformer(transformers=[\n",
    "    ('continuous', continuous_pipeline, X_train.columns),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:1.0\n",
      "Validation Score:0.9805750350631136\n"
     ]
    }
   ],
   "source": [
    "model_one = Pipeline(steps=[\n",
    "    ('trans', trans),\n",
    "    ('simple_dt', DecisionTreeClassifier(max_depth = 5, random_state = 42))\n",
    "])\n",
    "#Fit model on all the data\n",
    "model_one.fit(X_train, y_train)\n",
    "#Grab predictions and print precision\n",
    "y_pred = model_one.predict(X_train)\n",
    "print(\"Training Score:\" + str(accuracy_score(y_train, y_pred)))\n",
    "#Run a cross validation to test for overfitting\n",
    "scores = np.mean(cross_val_score(model_one, X_train, y_train, cv=5, scoring = 'accuracy'))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1B19 0.35259178673297703\n",
      "F1STSVPC 0.03081664371763838\n",
      "F1TUFEFT 0.26798418972332017\n",
      "F1E09 0.1955551086082057\n",
      "F1A14 0.0909712843581109\n",
      "F1OTEXFT 0.06208098685974785\n"
     ]
    }
   ],
   "source": [
    "for name, importance in zip(X_train.columns, model_one['simple_dt'].feature_importances_):\n",
    "    print(name, importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1B19</th>\n",
       "      <th>F1STSVPC</th>\n",
       "      <th>F1TUFEFT</th>\n",
       "      <th>F1E09</th>\n",
       "      <th>F1A14</th>\n",
       "      <th>F1OTEXFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1B19</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.518087</td>\n",
       "      <td>0.648069</td>\n",
       "      <td>0.597588</td>\n",
       "      <td>0.854113</td>\n",
       "      <td>0.271116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1STSVPC</th>\n",
       "      <td>-0.518087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.583611</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>-0.462333</td>\n",
       "      <td>-0.126459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1TUFEFT</th>\n",
       "      <td>0.648069</td>\n",
       "      <td>-0.583611</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.641629</td>\n",
       "      <td>0.690133</td>\n",
       "      <td>0.167303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1E09</th>\n",
       "      <td>0.597588</td>\n",
       "      <td>-0.366469</td>\n",
       "      <td>0.641629</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708646</td>\n",
       "      <td>0.107830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1A14</th>\n",
       "      <td>0.854113</td>\n",
       "      <td>-0.462333</td>\n",
       "      <td>0.690133</td>\n",
       "      <td>0.708646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.274963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1OTEXFT</th>\n",
       "      <td>0.271116</td>\n",
       "      <td>-0.126459</td>\n",
       "      <td>0.167303</td>\n",
       "      <td>0.107830</td>\n",
       "      <td>0.274963</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             F1B19  F1STSVPC  F1TUFEFT     F1E09     F1A14  F1OTEXFT\n",
       "F1B19     1.000000 -0.518087  0.648069  0.597588  0.854113  0.271116\n",
       "F1STSVPC -0.518087  1.000000 -0.583611 -0.366469 -0.462333 -0.126459\n",
       "F1TUFEFT  0.648069 -0.583611  1.000000  0.641629  0.690133  0.167303\n",
       "F1E09     0.597588 -0.366469  0.641629  1.000000  0.708646  0.107830\n",
       "F1A14     0.854113 -0.462333  0.690133  0.708646  1.000000  0.274963\n",
       "F1OTEXFT  0.271116 -0.126459  0.167303  0.107830  0.274963  1.000000"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-3fc4cefdca38>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-3fc4cefdca38>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    ('dummy', DummyClassifier(random_state = 42, strategy = ))\u001b[0m\n\u001b[1;37m                                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('si', SimpleImputer(missing_values=np.nan))\n",
    "])\n",
    "\n",
    "# column transformer\n",
    "trans = ColumnTransformer(transformers=[\n",
    "    ('pipeline', pipeline, X_train.columns)\n",
    "])\n",
    "\n",
    "#Pipeline for running the model\n",
    "dummy = Pipeline(steps=[\n",
    "    ('trans', trans),\n",
    "    ('dummy', DummyClassifier(random_state = 42, strategy = '' ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Bin and fill in nulls in installer\n",
    "inst_five = X_train.installer.value_counts(sort = True, ascending = False)[:5]\n",
    "inst_list = list(inst_five.index)\n",
    "for idx, value in enumerate(inst_list):\n",
    "    inst_list[idx] = value.lower()\n",
    "X_train['installer'] = X_train['installer'].apply(install_bin)\n",
    "X_test['installer'] = X_test['installer'].apply(install_bin)\n",
    "      \n",
    "#Bin and fill in nulls in scheme_management\n",
    "scheme_eight = X_train.scheme_management.value_counts(sort = True, ascending = False)[:9]\n",
    "scheme_list = list(scheme_eight.index)\n",
    "for idx, value in enumerate(scheme_list):\n",
    "    scheme_list[idx] = value.lower() \n",
    "X_train['scheme_management'] = X_train['scheme_management'].apply(scheme_bin)\n",
    "X_test['scheme_management'] = X_test['scheme_management'].apply(install_bin)\n",
    "\n",
    "#Create categorical and continuous feature split\n",
    "X_train_cat = X_train.select_dtypes('object')\n",
    "X_train_cont = X_train.select_dtypes(['float64', 'int64'])\n",
    "\n",
    "#Set up pipeline for scaling continuous variables\n",
    "continuous_pipeline = Pipeline(steps=[\n",
    "    ('ss', StandardScaler())\n",
    "])\n",
    "\n",
    "#Set up pipeline for encoding categorical variables\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('ohe', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "#Bind the scaling and encoding process together\n",
    "trans = ColumnTransformer(transformers=[\n",
    "    ('continuous', continuous_pipeline, X_train_cont.columns),\n",
    "    ('categorical', categorical_pipeline, X_train_cat.columns)\n",
    "])\n",
    "\n",
    "#Pipeline for running the model\n",
    "dummy = Pipeline(steps=[\n",
    "    ('trans', trans),\n",
    "    ('dummy', DummyClassifier(random_state = 42, strategy = 'most_frequent'))\n",
    "])\n",
    "\n",
    "#Fitting and checking the score\n",
    "dummy.fit(X_train, y_train)\n",
    "dummy.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline for decision tree\n",
    "model_one = Pipeline(steps=[\n",
    "    ('trans', trans),\n",
    "    ('simple_dt', DecisionTreeClassifier(max_depth = 5, random_state = 42))\n",
    "])\n",
    "\n",
    "#Fit model on all the data\n",
    "model_one.fit(X_train, y_train)\n",
    "#Grab predictions and print precision\n",
    "y_pred = model_one.predict(X_train)\n",
    "print(\"Training Score:\" + str(precision_score(y_train, y_pred)))\n",
    "#Run a cross validation to test for overfitting\n",
    "scores = np.mean(cross_val_score(model_one, X_train, y_train, cv=5, scoring = 'precision'))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, importance in zip(X_train.columns, model_one['simple_dt'].feature_importances_):\n",
    "    print(name, importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new features dataframe based on results above\n",
    "X = df_trim[['amount_tsh', 'permit', 'installer', 'extraction_type_class']]\n",
    "\n",
    "#Split the data again\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\n",
    "\n",
    "#Bin installer again\n",
    "inst_five = X_train.installer.value_counts(sort = True, ascending = False)[:5]\n",
    "inst_list = list(inst_five.index)\n",
    "for idx, value in enumerate(inst_list):\n",
    "    inst_list[idx] = value.lower()\n",
    "X_train['installer'] = X_train['installer'].apply(install_bin)\n",
    "X_test['installer'] = X_test['installer'].apply(install_bin)\n",
    "\n",
    "#Separate out which columns are categorical or continuous\n",
    "X_train_cat = ['permit', 'installer', 'extraction_type_class']\n",
    "X_train_cont = ['amount_tsh']\n",
    "\n",
    "#Adjust transformer to account for change in assigning X_train_cont\n",
    "trans = ColumnTransformer(transformers=[\n",
    "    ('continuous', continuous_pipeline, X_train_cont),\n",
    "    ('categorical', categorical_pipeline, X_train_cat)\n",
    "])\n",
    "\n",
    "#Pipeline for logistic regression\n",
    "logreg = Pipeline(steps=[\n",
    "    ('trans', trans),\n",
    "    ('logr', LogisticRegression(random_state = 42))\n",
    "])\n",
    "\n",
    "#Fit the model\n",
    "logreg.fit(X_train, y_train)\n",
    "#Print precision or training and validation sets\n",
    "y_pred = logreg.predict(X_train)\n",
    "print(\"Training Score:\" + str(precision_score(y_train, y_pred)))\n",
    "scores = np.mean(cross_val_score(logreg, X_train, y_train, cv=5, scoring = 'precision'))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logreg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-294e83cc703c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Grab probabilities and calculate log odds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlog_odds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Plot log odds versus continuous variable to check for linearity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'amount_tsh'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_odds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logreg' is not defined"
     ]
    }
   ],
   "source": [
    "#Grab probabilities and calculate log odds\n",
    "pred = logreg.predict_proba(X_train)[:, 0]\n",
    "log_odds = np.log(pred / (1 - pred))\n",
    "#Plot log odds versus continuous variable to check for linearity\n",
    "plt.scatter(x = X_train['amount_tsh'].values, y = log_odds)\n",
    "plt.title(\"Logistic Regression Assumption Test\")\n",
    "plt.xlabel(\"amount_tsh\")\n",
    "plt.ticklabel_format(axis='x', style='sci', scilimits=(0,0))\n",
    "plt.ylabel(\"Log-odds\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build pipeline for random forest\n",
    "ensemble = Pipeline(steps=[\n",
    "    ('trans', trans),\n",
    "    ('rfc', RandomForestClassifier(random_state = 42))\n",
    "])\n",
    "\n",
    "#Fit the model\n",
    "ensemble.fit(X_train, y_train)\n",
    "#Print out precision for training and validation\n",
    "y_pred = ensemble.predict(X_train)\n",
    "print(\"Training Score:\" + str(precision_score(y_train, y_pred)))\n",
    "scores = np.mean(cross_val_score(ensemble, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avoid a long runtime, the code is included but hashed out\n",
    "\"\"\"\n",
    "#Create parameters to test\n",
    "params = {\n",
    "    'rfc__criterion': ['gini', 'entropy'],\n",
    "    'rfc__n_estimators': [100, 300, 500],\n",
    "    'rfc__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "#Fit gridsearch on model and prints out the best parameters\n",
    "search = GridSearchCV(ensemble, param_grid = params, scoring = 'precision')\n",
    "search.fit(X_train, y_train)\n",
    "search.best_params_\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_tuned = Pipeline(steps=[\n",
    "    ('trans', trans),\n",
    "    ('rfc', RandomForestClassifier(criterion = 'entropy', min_samples_split = 5, \n",
    "                                   n_estimators = 300, random_state = 42))\n",
    "])\n",
    "\n",
    "#Fit and print precision for tuned model\n",
    "ensemble_tuned.fit(X_train, y_train)\n",
    "y_pred = ensemble_tuned.predict(X_train)\n",
    "print(\"Training Score:\" + str(precision_score(y_train, y_pred)))\n",
    "scores = np.mean(cross_val_score(ensemble_tuned, X_train, y_train, cv=5))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Accuracy:\" + str(logreg.score(X_train, y_train)))\n",
    "scores = np.mean(cross_val_score(logreg, X_train, y_train, cv=5))\n",
    "print(\"Validation Accuracy:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate precision score for test set\n",
    "test_pred = logreg.predict(X_test)\n",
    "print(\"Test Score:\" + str(precision_score(y_test, test_pred)))\n",
    "print(\"Tets Accuracy:\" + str(logreg.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f697dcecb1d05ac00f1a3e6eaa77fcadd261c808812655cf66e71c69fa75e0c6"
  },
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
