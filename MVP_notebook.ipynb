{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Student Success is often measured by 150% graduation rate (citation). The racial disparities in these numbers show the postsecondary educational gaps. Colleges will want to increase their black student success rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "Colleges who puport to help low income and minority students are often in a tight spot. Because they are putting extra dollars to help those students without, their margins are thinner. Consequently their graduation rates, or student sucess rates are lower than private schools who benefit from selecting better students and supporting them with wealthier students tuition or endowments. \n",
    "\n",
    "In pilling through IPEDS instiutional financial data, this project offers financial advice to public universities who are struggling to graduate black students, or would like to do better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "Limitations. The graduation rate for IPEDS only calculates for first time, full-time students. Excludes transfers, winter enrollment, and part-timers. Instituion focused, not the individual students. hence the before, only students who start and finish at the university are included. \n",
    "No income data on students...can't focus on low income students. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import pyodbc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from functools import reduce\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import  accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IPEDS is from the Integrated Postsecondary Educatnoi Data System, accessed through yearly Microsoft Access databases and pyodbc. Six years (2014-2019) of financial data from 1700 public and private universities were pulled. After the queries were saved to csv, they are pulled to dataframe here. One advantage of IPEDS is that all tables have the same key: 'UNITID,' except for the provisional tables in 2019-2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the connection\n",
    "conn_str = (r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'\n",
    "            r'DBQ=data/IPEDS201920.accdb;')\n",
    "conn= pyodbc.connect(conn_str)\n",
    "#example: pulling the target variable\n",
    "df = pd.read_sql('select UNITID, GBA6RTT from DRVGR2019', conn)\n",
    "# #save to csv, example \"target_2019\"\n",
    "df.to_csv(\"data/target_2019.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull financials\n",
    "df_private = pd.read_csv(\"data/private_financials.csv\").iloc[:, 1:]\n",
    "df_public = pd.read_csv(\"data/public_financials.csv\").iloc[:, 1:]\n",
    "df_ids = pd.read_csv(\"data/university_ids.csv\").iloc[:,0:2]\n",
    "df_ids.rename(columns={'IPEDS\\nUnit ID': 'UNITID', 'Organization or School Name':'School'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the financial columns are just the same data in relation to full time employers (FTE)(ending in FT) or in percentage of revenue/expenses (ending in PC). These columns are typically collinear. This project chose those FT columns, because of the efficiency of the metric in comparing universities per full time employees. \n",
    "\n",
    "Getting rid of columns such as, but not limited to, F1TUFEPC, F1STAPPC, F1LCAPPC, F1GVGCPC, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "combining  income (from something other than tuition), and instruction, research, and institutional support and endowment expenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNITID</th>\n",
       "      <th>F2A01</th>\n",
       "      <th>F2A19</th>\n",
       "      <th>F2A20</th>\n",
       "      <th>F2A02</th>\n",
       "      <th>F2A03</th>\n",
       "      <th>F2A03A</th>\n",
       "      <th>F2A04</th>\n",
       "      <th>F2A05</th>\n",
       "      <th>F2A05A</th>\n",
       "      <th>...</th>\n",
       "      <th>F2E115</th>\n",
       "      <th>F2E116</th>\n",
       "      <th>F2E117</th>\n",
       "      <th>F2E123</th>\n",
       "      <th>F2E124</th>\n",
       "      <th>F2E125</th>\n",
       "      <th>F2E126</th>\n",
       "      <th>F2E127</th>\n",
       "      <th>Research_and_endowment</th>\n",
       "      <th>Academic_and_student_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100690</td>\n",
       "      <td>9384788.0</td>\n",
       "      <td>2634979.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14438530.0</td>\n",
       "      <td>2487355.0</td>\n",
       "      <td>1296697.0</td>\n",
       "      <td>11242127.0</td>\n",
       "      <td>709048.0</td>\n",
       "      <td>174805.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100937</td>\n",
       "      <td>50743827.0</td>\n",
       "      <td>89400865.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159285516.0</td>\n",
       "      <td>47756472.0</td>\n",
       "      <td>34000110.0</td>\n",
       "      <td>53054909.0</td>\n",
       "      <td>58474135.0</td>\n",
       "      <td>7669338.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101189</td>\n",
       "      <td>20868071.0</td>\n",
       "      <td>53530199.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89941892.0</td>\n",
       "      <td>42026935.0</td>\n",
       "      <td>33512123.0</td>\n",
       "      <td>29345184.0</td>\n",
       "      <td>18569773.0</td>\n",
       "      <td>13162812.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3242987.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9435073.0</td>\n",
       "      <td>3717479.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5717594.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101435</td>\n",
       "      <td>50818280.0</td>\n",
       "      <td>29298003.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86309501.0</td>\n",
       "      <td>26153792.0</td>\n",
       "      <td>22307594.0</td>\n",
       "      <td>9068001.0</td>\n",
       "      <td>51087708.0</td>\n",
       "      <td>47866409.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9344</th>\n",
       "      <td>459842</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9345</th>\n",
       "      <td>459851</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9346</th>\n",
       "      <td>460349</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>485077.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9347</th>\n",
       "      <td>475228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9348</th>\n",
       "      <td>482705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19921836.0</td>\n",
       "      <td>3727841.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9349 rows × 231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      UNITID       F2A01       F2A19  F2A20        F2A02       F2A03  \\\n",
       "0     100690   9384788.0   2634979.0    0.0   14438530.0   2487355.0   \n",
       "1     100937  50743827.0  89400865.0    0.0  159285516.0  47756472.0   \n",
       "2     101189  20868071.0  53530199.0    0.0   89941892.0  42026935.0   \n",
       "3     101365         0.0   3242987.0    0.0    9435073.0   3717479.0   \n",
       "4     101435  50818280.0  29298003.0    0.0   86309501.0  26153792.0   \n",
       "...      ...         ...         ...    ...          ...         ...   \n",
       "9344  459842         NaN         NaN    NaN          NaN         NaN   \n",
       "9345  459851         NaN         NaN    NaN          NaN         NaN   \n",
       "9346  460349         NaN         NaN    NaN          NaN         NaN   \n",
       "9347  475228         NaN         NaN    NaN          NaN         NaN   \n",
       "9348  482705         NaN         NaN    NaN          NaN         NaN   \n",
       "\n",
       "          F2A03A       F2A04       F2A05      F2A05A  ...  F2E115  F2E116  \\\n",
       "0      1296697.0  11242127.0    709048.0    174805.0  ...     NaN     NaN   \n",
       "1     34000110.0  53054909.0  58474135.0   7669338.0  ...     NaN     NaN   \n",
       "2     33512123.0  29345184.0  18569773.0  13162812.0  ...     NaN     NaN   \n",
       "3            0.0   5717594.0         0.0         0.0  ...     NaN     NaN   \n",
       "4     22307594.0   9068001.0  51087708.0  47866409.0  ...     NaN     NaN   \n",
       "...          ...         ...         ...         ...  ...     ...     ...   \n",
       "9344         NaN         NaN         NaN         NaN  ...     NaN     NaN   \n",
       "9345         NaN         NaN         NaN         NaN  ...     NaN     NaN   \n",
       "9346         NaN         NaN         NaN         NaN  ...     NaN     NaN   \n",
       "9347         NaN         NaN         NaN         NaN  ...     NaN     NaN   \n",
       "9348         NaN         NaN         NaN         NaN  ...     NaN     NaN   \n",
       "\n",
       "      F2E117  F2E123  F2E124  F2E125  F2E126  F2E127  Research_and_endowment  \\\n",
       "0        NaN     NaN     NaN     NaN     NaN     NaN                     NaN   \n",
       "1        NaN     NaN     NaN     NaN     NaN     NaN                     NaN   \n",
       "2        NaN     NaN     NaN     NaN     NaN     NaN                     NaN   \n",
       "3        NaN     NaN     NaN     NaN     NaN     NaN                     NaN   \n",
       "4        NaN     NaN     NaN     NaN     NaN     NaN                     NaN   \n",
       "...      ...     ...     ...     ...     ...     ...                     ...   \n",
       "9344     NaN     NaN     NaN     NaN     NaN     NaN                     NaN   \n",
       "9345     NaN     NaN     NaN     NaN     NaN     NaN                     NaN   \n",
       "9346     NaN     NaN     NaN     NaN     NaN     NaN                     0.0   \n",
       "9347     NaN     NaN     NaN     NaN     NaN     NaN                     0.0   \n",
       "9348     NaN     NaN     NaN     NaN     NaN     NaN              19921836.0   \n",
       "\n",
       "      Academic_and_student_support  \n",
       "0                              NaN  \n",
       "1                              NaN  \n",
       "2                              NaN  \n",
       "3                              NaN  \n",
       "4                              NaN  \n",
       "...                            ...  \n",
       "9344                           NaN  \n",
       "9345                           NaN  \n",
       "9346                      485077.0  \n",
       "9347                           0.0  \n",
       "9348                     3727841.0  \n",
       "\n",
       "[9349 rows x 231 columns]"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#public income other than tuition -- F1GVGCFT, F1PGGCFT, F1INVRFT, F1OTRVFT\n",
    "df_public['Rev_not_tuition'] =df_public['F1GVGCFT'] * df_public['F1PGGCFT'] *df_public['F1INVRFT'] *df_public['F1OTRVFT']\n",
    "df_public.drop(columns=['F1GVGCFT', 'F1PGGCFT', 'F1INVRFT', 'F1OTRVFT'], axis=1)\n",
    "\n",
    "#public instructional expenses -- F1INSTFT, F1RSRCFT,F1INSUFT, F1ENDMFT\n",
    "df_public['Instruction_and_research'] =df_public['F1INSTFT'] *df_public['F1RSRCFT'] *df_public['F1INSUFT'] *df_public['F1ENDMFT']\n",
    "df_public.drop(columns=['F1INSTFT', 'F1RSRCFT','F1INSUFT', 'F1ENDMFT'], axis=1)\n",
    "\n",
    "#private research and endowment expenses -- F2RSRCFT,F2ENDMFT\n",
    "df_private['Research_and_endowment']= df_private['F2RSRCFT'] *df_private['F2ENDMFT']\n",
    "df_private.drop(columns=['F2RSRCFT','F2ENDMFT'], axis=1)\n",
    "\n",
    "#private student support -- F2ACSPFT, F2STSVFT\n",
    "df_private['Academic_and_student_support']= df_private['F2ACSPFT'] *df_private['F2STSVFT']\n",
    "df_private.drop(columns=['F2ACSPFT', 'F2STSVFT'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "#University Labels\n",
    "private_ids = [df_ids, df_private]\n",
    "public_ids = [df_ids, df_public]\n",
    "df_private= reduce(lambda x, y: pd.merge(x, y, on = 'UNITID'), private_ids)\n",
    "df_public= reduce(lambda x, y: pd.merge(x, y, on = 'UNITID'), public_ids)\n",
    "df_private.set_index(['School'], inplace=True)\n",
    "df_public.set_index(['School'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the target was not present, the observation was droped. \n",
    "also, if the row or column contained no information, it was droped. \n",
    "Once the index was set to year and the ID, all nans were droped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you don't have the target, why are you even here?\n",
    "df_private = df_private[~df_private['GBA6RTBK'].isna()]\n",
    "#and again for public\n",
    "df_public = df_public[~df_public['GBA6RTBK'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "#private\n",
    "y_private_continuous = df_private['GBA6RTBK']\n",
    "X_private_continuous = df_private.drop(columns = ['GBA6RTBK'], axis = 1)\n",
    "X_train_private_continuous, X_test_private_continuous, y_train_private_continuous, y_test_private_continuous = train_test_split(X_private_continuous, y_private_continuous, random_state = 42)\n",
    "#and for public\n",
    "y_public_continuous = df_public['GBA6RTBK']\n",
    "X_public_continuous = df_public.drop(columns = ['GBA6RTBK'], axis = 1)\n",
    "X_train_public_continuous, X_test_public_continuous, y_train_public_continuous, y_test_public_continuous = train_test_split(X_public_continuous, y_public_continuous, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target-- ternanry and continuous\n",
    "classification and regressoion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ternary bining\n",
    "bin_labels =['low', 'medium', 'high']\n",
    "df_private['gr_rank'] = pd.qcut(df_private['GBA6RTBK'], q=3, labels=bin_labels)\n",
    "#for public\n",
    "df_public['gr_rank'] = pd.qcut(df_public['GBA6RTBK'], q=3, labels=bin_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "#private\n",
    "y_private = df_private['gr_rank']\n",
    "X_private = df_private.drop(columns = ['GBA6RTBK', 'gr_rank'], axis = 1)\n",
    "X_train_private, X_test_private, y_train_private, y_test_private = train_test_split(X_private, y_private, random_state = 42)\n",
    "#and for public\n",
    "y_public = df_public['gr_rank']\n",
    "X_public = df_public.drop(columns = ['GBA6RTBK', 'gr_rank'], axis = 1)\n",
    "X_train_public, X_test_public, y_train_public, y_test_public = train_test_split(X_public, y_public, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ternary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33511111111111114"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set up pipeline for scaling continuous variables\n",
    "pipeline_private= Pipeline(steps=[\n",
    "    ('si', SimpleImputer()),\n",
    "    ('ss', StandardScaler())\n",
    "])\n",
    "#Pipeline for running the model\n",
    "dummy_private = Pipeline(steps=[\n",
    "    ('pip', pipeline_private),\n",
    "    ('dummy', DummyClassifier(random_state = 42))\n",
    "])\n",
    "#Fitting and checking the score\n",
    "dummy_private.fit(X_train_private, y_train_private)\n",
    "dummy_private.score(X_train_private, y_train_private)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('pip',\n",
       "                 Pipeline(steps=[('si', SimpleImputer()),\n",
       "                                 ('ss', StandardScaler())])),\n",
       "                ('simple_dt',\n",
       "                 DecisionTreeClassifier(max_depth=5, random_state=42))])"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_one_private = Pipeline(steps=[\n",
    "    ('pip', pipeline_private),\n",
    "    ('simple_dt', DecisionTreeClassifier(max_depth = 5, random_state = 42))\n",
    "])\n",
    "#Fit model on all the data\n",
    "model_one_private.fit(X_train_private, y_train_private)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3442113442113442"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for public\n",
    "pipeline_public = Pipeline(steps=[\n",
    "    ('si', SimpleImputer()),\n",
    "    ('ss', StandardScaler())\n",
    "])\n",
    "#Public\n",
    "dummy_public = Pipeline(steps=[\n",
    "    ('pip', pipeline_public),\n",
    "    ('dummy', DummyClassifier(random_state = 42))\n",
    "])\n",
    "#Fitting and checking the score\n",
    "dummy_public.fit(X_train_public, y_train_public)\n",
    "dummy_public.score(X_train_public, y_train_public)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('pip',\n",
       "                 Pipeline(steps=[('si', SimpleImputer()),\n",
       "                                 ('ss', StandardScaler())])),\n",
       "                ('simple_dt',\n",
       "                 DecisionTreeClassifier(max_depth=5, random_state=42))])"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_one_public = Pipeline(steps=[\n",
    "    ('pip', pipeline_public),\n",
    "    ('simple_dt', DecisionTreeClassifier(max_depth = 5, random_state = 42))\n",
    "])\n",
    "\n",
    "#Fit model on all the data\n",
    "model_one_public.fit(X_train_public, y_train_public)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:0.6551111111111111\n",
      "Validation Score:0.6057777777777777\n"
     ]
    }
   ],
   "source": [
    "#Grab predictions and print precision\n",
    "y_pred_private = model_one_private.predict(X_train_private)\n",
    "print(\"Training Score:\" + str(accuracy_score(y_train_private, y_pred_private)))\n",
    "#Run a cross validation to test for overfitting\n",
    "scores_private = np.mean(cross_val_score(model_one_private, X_train_private, y_train_private, cv=5, scoring = 'accuracy'))\n",
    "print(\"Validation Score:\" + str(scores_private))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:0.6507381507381508\n",
      "Validation Score:0.5842869555362473\n"
     ]
    }
   ],
   "source": [
    "#Grab predictions and print precision\n",
    "y_pred_public = model_one_public.predict(X_train_public)\n",
    "print(\"Training Score:\" + str(accuracy_score(y_train_public, y_pred_public)))\n",
    "#Run a cross validation to test for overfitting\n",
    "scores_public = np.mean(cross_val_score(model_one_public, X_train_public, y_train_public, cv=5, scoring = 'accuracy'))\n",
    "print(\"Validation Score:\" + str(scores_public))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_tuple(tup):\n",
    "    tup.sort(reverse=True, key=lambda x:x[1])\n",
    "    return tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('F2A06', 0.4653450055914487), ('F2A18', 0.12289419261027125), ('F2A05A', 0.09496353262754326), ('F2INSTFT', 0.09160673141915081), ('F2C01', 0.03788333731547315), ('F2C08', 0.037537985051071095), ('F2E133', 0.02576173929007994), ('F2CORREV', 0.016276470253849425), ('F2E041', 0.011763869624225716), ('F2H01', 0.011193182155989004), ('F2TUFEFT', 0.01073886132953051), ('F2D083B', 0.010360784457951255), ('F2E071', 0.009781959535522562), ('F2A05', 0.009542115680950609), ('F2STSVFT', 0.008529016164702157), ('F2E131', 0.006400055635056215), ('F2E012', 0.006329523389921599), ('F2A03', 0.006251877026941996), ('F2ACSPPC', 0.005721576405568242), ('F2E056', 0.005368992421025483), ('F2B07', 0.0019924113164072887), ('F2E107', 0.001983729785616787), ('F2D05', 0.0017730509117029683)]\n"
     ]
    }
   ],
   "source": [
    "#Feature Importance\n",
    "important_private = []\n",
    "names_private = []\n",
    "for name, importance in zip(X_train_private.columns, model_one_private['simple_dt'].feature_importances_):\n",
    "    if importance > 0:\n",
    "        important_private.append((name, importance))\n",
    "        names_private.append(name)\n",
    "\n",
    "print(sort_tuple(important_private))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('F1B05', 0.3449286397829274), ('F1C072', 0.19057652606475456), ('F1E01', 0.08546957972369444), ('F1A10', 0.07128870676181848), ('F1INSTFT', 0.039343018410130264), ('F1E09', 0.029257810835965), ('F1C061', 0.02898491424146599), ('F1TUFEFT', 0.02646382128737476), ('F1A284', 0.024537955144165104), ('F1B11', 0.022995867540547683), ('F1B13', 0.02085825019856816), ('F1C022', 0.019212625912713905), ('F1E02', 0.019024781862010388), ('F1E03', 0.01898452452669583), ('F1STAPFT', 0.013328816136141767), ('F1C112', 0.011688313588558636), ('F1M02', 0.008532699234351772), ('F1A214', 0.005235584652843107), ('UNITID', 0.004681554530849134), ('F1OTRVFT', 0.004356798576361169), ('F1ACSPPC', 0.003440527057582614), ('F1C013', 0.003425486946390443), ('F1A344', 0.0033831969840893024)]\n"
     ]
    }
   ],
   "source": [
    "#For Public\n",
    "important_public = []\n",
    "names_public = []\n",
    "for name, importance in zip(X_train_public.columns, model_one_public['simple_dt'].feature_importances_):\n",
    "    if importance > 0:\n",
    "        important_public.append((name, importance))\n",
    "        names_public.append(name)\n",
    "\n",
    "print(sort_tuple(important_public))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F2A03',\n",
       " 'F2A05',\n",
       " 'F2A05A',\n",
       " 'F2A06',\n",
       " 'F2A18',\n",
       " 'F2B07',\n",
       " 'F2C01',\n",
       " 'F2C08',\n",
       " 'F2D05',\n",
       " 'F2D083B',\n",
       " 'F2E012',\n",
       " 'F2E041',\n",
       " 'F2E071',\n",
       " 'F2E131',\n",
       " 'F2E133',\n",
       " 'F2H01',\n",
       " 'F2CORREV',\n",
       " 'F2TUFEFT',\n",
       " 'F2ACSPPC',\n",
       " 'F2INSTFT',\n",
       " 'F2STSVFT',\n",
       " 'F2E056',\n",
       " 'F2E107']"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_private"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_private = X_train_private[['F2A03',\n",
    " 'F2A05',\n",
    " 'F2A05A',\n",
    " 'F2A06',\n",
    " 'F2A18',\n",
    " 'F2B07',\n",
    " 'F2C01',\n",
    " 'F2C08',\n",
    " 'F2D05',\n",
    " 'F2D083B',\n",
    " 'F2E012',\n",
    " 'F2E041',\n",
    " 'F2E071',\n",
    " 'F2E131',\n",
    " 'F2E133',\n",
    " 'F2H01',\n",
    " 'F2CORREV',\n",
    " 'F2TUFEFT',\n",
    " 'F2ACSPPC',\n",
    " 'F2INSTFT',\n",
    " 'F2STSVFT',\n",
    " 'F2E056',\n",
    " 'F2E107']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNITID',\n",
       " 'F1A10',\n",
       " 'F1A214',\n",
       " 'F1A284',\n",
       " 'F1A344',\n",
       " 'F1B05',\n",
       " 'F1B11',\n",
       " 'F1B13',\n",
       " 'F1C022',\n",
       " 'F1C061',\n",
       " 'F1C072',\n",
       " 'F1C112',\n",
       " 'F1M02',\n",
       " 'F1E01',\n",
       " 'F1E02',\n",
       " 'F1E03',\n",
       " 'F1E09',\n",
       " 'F1TUFEFT',\n",
       " 'F1STAPFT',\n",
       " 'F1OTRVFT',\n",
       " 'F1ACSPPC',\n",
       " 'F1INSTFT',\n",
       " 'F1C013']"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_public = X_train_public[[ 'F1A10',\n",
    " 'F1A214',\n",
    " 'F1A284',\n",
    " 'F1A344',\n",
    " 'F1B05',\n",
    " 'F1B11',\n",
    " 'F1B13',\n",
    " 'F1C022',\n",
    " 'F1C061',\n",
    " 'F1C072',\n",
    " 'F1C112',\n",
    " 'F1M02',\n",
    " 'F1E01',\n",
    " 'F1E02',\n",
    " 'F1E03',\n",
    " 'F1E09',\n",
    " 'F1TUFEFT',\n",
    " 'F1STAPFT',\n",
    " 'F1OTRVFT',\n",
    " 'F1ACSPPC',\n",
    " 'F1INSTFT',\n",
    " 'F1C013']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:0.6171111111111112\n",
      "Validation Score:0.6119999999999999\n"
     ]
    }
   ],
   "source": [
    "logreg_private = Pipeline(steps=[\n",
    "    ('cont', pipeline_private),\n",
    "    ('logr', LogisticRegression(multi_class=\"multinomial\"))\n",
    "])\n",
    "#Fit model on all the data\n",
    "logreg_private.fit(X_train_private, y_train_private)\n",
    "#Grab predictions and print precision\n",
    "y_pred_private = logreg_private.predict(X_train_private)\n",
    "print(\"Training Score:\" + str(logreg_private.score(X_train_private,y_train_private)))\n",
    "scores = np.mean(cross_val_score(logreg_private, X_train_private, y_train_private, cv=5, scoring = 'accuracy'))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1765165   0.54070401  1.42950564  0.33758491  1.51750933  0.33758491\n",
      "  -0.76216375  0.96171881 -0.35051187  0.42768805 -1.30284806 -0.0172185\n",
      "   0.98733112  0.41343881  0.13790308  1.62204778 -0.11611698  0.26568791\n",
      "   0.0768435   0.50037792  0.00262768 -0.00970094 -0.28035849]\n",
      " [ 0.03804075 -1.25416526 -2.16529802  1.2235692  -1.82802419  1.2235692\n",
      "   0.50211092 -1.35816759  0.19019193 -0.35371857  0.51584012 -0.35810975\n",
      "  -1.12865214 -0.85418755 -0.02641991 -0.37462479  0.34725321 -0.22702243\n",
      "  -0.04366786 -0.42332757  0.02892833  0.15092552  0.26357733]\n",
      " [ 0.13847574  0.71346124  0.73579237 -1.56115411  0.31051486 -1.56115411\n",
      "   0.26005282  0.39644878  0.16031994 -0.07396948  0.78700794  0.37532824\n",
      "   0.14132101  0.44074874 -0.11148317 -1.24742299 -0.23113624 -0.03866549\n",
      "  -0.03317564 -0.07705034 -0.03155602 -0.14122457  0.01678116]]\n"
     ]
    }
   ],
   "source": [
    "print(logreg_private[\"logr\"].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:0.6064491064491064\n",
      "Validation Score:0.5967239620717011\n"
     ]
    }
   ],
   "source": [
    "logreg_public = Pipeline(steps=[\n",
    "    ('cont', pipeline_public),\n",
    "    ('logr', LogisticRegression(multi_class=\"multinomial\"))\n",
    "])\n",
    "#Fit model on all the data\n",
    "logreg_public.fit(X_train_public, y_train_public)\n",
    "#Grab predictions and print precision\n",
    "y_pred_public = logreg_public.predict(X_train_public)\n",
    "print(\"Training Score:\" + str(logreg_public.score(X_train_public,y_train_public)))\n",
    "scores = np.mean(cross_val_score(logreg_public,X_train_public, y_train_public, cv=5, scoring = 'accuracy'))\n",
    "print(\"Validation Score:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.13542086e-01 -1.64848302e-01 -9.26367929e-01 -3.64199840e-02\n",
      "   1.52745333e+00  1.20158098e-01 -1.57263166e-01  5.21209129e-02\n",
      "   2.78654558e-01  5.58684811e-01 -2.26641023e-01  1.17447053e-01\n",
      "  -2.45413304e-01  1.32604823e-03  5.77635633e-01  3.26524223e-01\n",
      "   1.68245617e-01  1.77235487e-01  1.81271421e-01  1.05870244e-01\n",
      "   8.81520139e-02 -1.48490622e-01]\n",
      " [-4.47272838e-01  2.85256083e-01  1.04360251e+00 -3.53823662e-03\n",
      "  -2.26338875e+00 -8.47321385e-02 -5.47604966e-02 -5.67484464e-02\n",
      "  -2.16090842e-01 -3.73006830e-01  3.26174106e-01 -2.80759311e-01\n",
      "   2.42347357e-01  8.03873912e-02 -7.36827107e-01 -2.47806598e-01\n",
      "  -2.51937889e-01 -1.53032566e-01 -2.72019928e-01 -2.98712116e-02\n",
      "  -1.33743850e-01  7.58058010e-02]\n",
      " [ 3.37307519e-02 -1.20407781e-01 -1.17234583e-01  3.99582206e-02\n",
      "   7.35935422e-01 -3.54259599e-02  2.12023662e-01  4.62753355e-03\n",
      "  -6.25637153e-02 -1.85677981e-01 -9.95330833e-02  1.63312258e-01\n",
      "   3.06594692e-03 -8.17134394e-02  1.59191475e-01 -7.87176249e-02\n",
      "   8.36922721e-02 -2.42029206e-02  9.07485069e-02 -7.59990319e-02\n",
      "   4.55918358e-02  7.26848209e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(logreg_public[\"logr\"].coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:0.41865974902826686\n",
      "Validation Score:-4.1059669609815754e+26\n"
     ]
    }
   ],
   "source": [
    "#Set up pipeline for scaling continuous variables\n",
    "continuous_pipeline_private= Pipeline(steps=[\n",
    "    ('si', SimpleImputer()),\n",
    "    ('ss', StandardScaler())\n",
    "])\n",
    "lr_private = Pipeline(steps=[\n",
    "    ('cont', continuous_pipeline_private),\n",
    "    ('ols', LinearRegression())\n",
    "])\n",
    "#Fit model on all the data\n",
    "lr_private.fit(X_train_private_continuous, y_train_private_continuous)\n",
    "#Grab predictions and print precision\n",
    "y_pred_private_continuous = lr_private.predict(X_train_private_continuous)\n",
    "print(\"Training Score:\" + str(lr_private.score(X_train_private_continuous, y_train_private_continuous)))\n",
    "#Run a cross validation to test for overfitting\n",
    "scores_private_continuous = np.mean(cross_val_score(lr_private, X_train_private_continuous, y_train_private_continuous, cv=5))\n",
    "print(\"Validation Score:\" + str(scores_private_continuous))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:0.49743286899784944\n",
      "Validation Score:-6842506773336856.0\n"
     ]
    }
   ],
   "source": [
    "#for public\n",
    "continuous_pipeline_public = Pipeline(steps=[\n",
    "    ('si', SimpleImputer()),\n",
    "    ('ss', StandardScaler())\n",
    "])\n",
    "lr_public = Pipeline(steps=[\n",
    "    ('cont', continuous_pipeline_public),\n",
    "    ('ols', LinearRegression())\n",
    "])\n",
    "#Fit model on all the data\n",
    "lr_public.fit(X_train_public_continuous, y_train_public_continuous)\n",
    "#Grab predictions and print precision\n",
    "y_pred_private_continuous = lr_public.predict(X_train_public_continuous)\n",
    "print(\"Training Score:\" + str(lr_public.score(X_train_public_continuous, y_train_public_continuous)))\n",
    "#Run a cross validation to test for overfitting\n",
    "scores_public_continuous = np.mean(cross_val_score(lr_public, X_train_public_continuous, y_train_public_continuous, cv=5))\n",
    "print(\"Validation Score:\" + str(scores_public_continuous))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('F2D162', 40363470549928.875), ('F2A06', 35942943648374.49), ('F2D07', 35739061716768.11), ('F2D172', 21724534483687.773), ('F2D173', 14895185779081.898), ('F2E114', 12448795702367.916), ('F2A05B', 10387772384558.506), ('F2D022', 10229574102656.969), ('F2D183', 9201579590100.605), ('F2D163', 6850868398863.648), ('F2D164', 6677756852333.558), ('F2E091', 6098380546000.099), ('F2D083', 6085488053525.62), ('F2D08B', 5816084553419.051), ('F2D15', 5497146782054.066), ('F2A12', 5386692394558.335), ('F2D05', 5142420176619.747), ('F2D102', 5067302827806.227), ('F2A03', 5032860485421.64), ('F2D09', 4881978209902.959), ('F2D13', 4853935907191.285), ('F2D04', 4741391253186.146), ('F2D06', 4740778399283.527), ('F2E011', 4333064871103.756), ('F2C06', 4304519046949.309), ('F2E014', 4235454057345.305), ('F2A05A', 4079451370076.6523), ('F2D084A', 3195557012965.509), ('F2E021', 3022178499865.8975), ('F2D084', 2893644178780.6094), ('F2E024', 2644364822671.117), ('F2D03', 2579668949792.756), ('F2E064', 2145898565815.1729), ('F2D082A', 2075547375076.456), ('F2D012', 1997233402937.418), ('F2E074', 1969268979485.4453), ('F2E101', 1918904323977.5278), ('F2A04', 1893532980474.904), ('F2C05', 1811736985043.7158), ('F2D14', 1793088868485.8152), ('F2E044', 1600390382152.5098), ('F2E121', 1599214935102.892), ('F2D104', 1460154618017.6877), ('F2A13', 1450269123304.1362), ('F2E041', 1450191855166.837), ('F2E061', 1362891053564.5754), ('F2E071', 1099482772203.1025), ('F2E094', 1056947912340.0967), ('F2E054', 973493482632.0447), ('F2E117', 955195329503.8733), ('F2D122', 858593823927.2905), ('F2D154', 777109006047.041), ('F2C01', 635579725684.7947), ('F2E051', 619898361536.6722), ('F2A15', 602081835276.0693), ('F2D174', 527432174037.43713), ('F2E112', 406434191106.0571), ('F2A11', 373765209043.23083), ('F2E124', 362992338947.7776), ('F2D08', 285631681305.8703), ('F2C02', 254873940494.77448), ('F2A16', 242059160090.8944), ('F2E031', 239304488155.83417), ('F2C03', 212521425142.37183), ('F2E081', 195047378284.30225), ('F2D17', 188372823216.73734), ('F2D034', 173854145171.82486), ('F2E034', 167239495986.5973), ('F2E115', 152146375909.7666), ('F2D11', 143273639686.11847), ('F2E116', 137334031214.35156), ('F2E113', 135752549774.46582), ('F2D023', 130867195013.30429), ('F2E104', 69075225507.84119), ('F2C04', 20203044440.661133), ('F2D054', 11776389961.282288), ('F2D144', 7330218044.370071), ('F2D014', 24339528.45689392), ('F2B01', 7330008.480957031), ('F2A19', 1973828.168728989), ('F2A18', 1459705.1810302734), ('F2B03', 255718.6055908203), ('F2RSRCFT', 17.5560302734375), ('F2COREXP', 13.0294189453125), ('F2E093', 11.92138671875), ('F2E095', 9.9405517578125), ('F2E105', 9.15576171875), ('F2E123', 7.8873291015625), ('F2E023', 7.8701171875), ('F2E126', 7.014404296875), ('F2E025', 5.93994140625), ('F2C08', 5.918548583984375), ('F2INVRPC', 5.65234375), ('F2TUFEFT', 4.88250732421875), ('F2TUFEPC', 4.836669921875), ('F2E015', 4.55120849609375), ('F2PGGCPC', 3.5906982421875), ('F2ENDMFT', 3.4453125), ('F2PBSVFT', 3.423095703125), ('F2E045', 3.2467041015625), ('F2E052', 3.1541748046875), ('F2GVGCPC', 2.79736328125), ('F2STSVFT', 2.76953125), ('F2E077', 2.646728515625), ('F2OTRVPC', 2.6357421875), ('F2E047', 2.555908203125), ('F2E057', 2.494415283203125), ('F2INSTFT', 2.39013671875), ('F2E056', 2.07568359375), ('F2E075', 1.8843994140625), ('F2INSUFT', 1.5853271484375), ('F2E035', 1.56939697265625), ('F2E096', 1.5345458984375), ('F2E032', 1.5028076171875), ('F2A03A', 0.9988836311991873), ('F2E066', 0.92529296875), ('F2ACSPPC', 0.834228515625), ('F2EQUITR', 0.81689453125), ('F2E065', 0.810302734375), ('F2E037', 0.79638671875), ('F2PGGCFT', 0.77392578125), ('F2A20', 0.6608958723095246), ('F2ACSPFT', 0.5443115234375), ('F2SAFBPC', 0.410888671875), ('F2INSTPC', 0.3251953125), ('F2FHA', 0.2919921875), ('F2OTEXPC', 0.21923828125), ('F2E017', 0.187255859375), ('F2E026', 0.16552734375)]\n"
     ]
    }
   ],
   "source": [
    "#Feature Importance\n",
    "important_private = []\n",
    "for name, coef in zip(X_train_private_continuous.columns, lr_private['ols'].coef_):\n",
    "    if coef > 0:\n",
    "        important_private.append((name, coef))\n",
    "print(sort_tuple(important_private))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('F1A05', 8964397430443.81), ('F1C086', 2125143990970.2334), ('F1A09', 1879835942888.337), ('F1C197', 1856894066247.9224), ('F1E07', 1812561832022.8342), ('F1E10', 1361116148622.3179), ('F1C085', 1328348886439.0034), ('F1A18', 1267095783864.6028), ('F1A01', 1173137352259.0464), ('F1A11', 934971413145.3087), ('F1C191', 870797342498.3925), ('F1B23', 763103044490.8269), ('F1A10', 714598925507.0208), ('F1D06', 603045753801.1238), ('F1C082', 517866443133.72595), ('F1B19', 448720636497.30255), ('F1A234', 436014711776.7397), ('F1C122', 360227593087.8633), ('F1B20', 304512827356.0935), ('F1C012', 292000311437.3684), ('F1B21', 277132569651.4617), ('F1C083', 226000359372.84644), ('F1B04', 194798910530.41852), ('F1C124', 177253197246.1604), ('F1C125', 172971141110.74878), ('F1B22', 161326213135.8686), ('F1C114', 157973741492.63428), ('F1C022', 157096938979.10303), ('F1C014', 153349082044.46875), ('F1A324', 135534520903.70837), ('F1B06', 135148846753.92346), ('F1C084', 135062999938.89877), ('F1C024', 120368198676.27344), ('F1B09', 113989146226.98273), ('F1D01', 111617925055.94537), ('F1C19OT', 100348643023.71753), ('F1C016', 99298726441.51453), ('F1C032', 95479083907.57736), ('F1C052', 85953751692.12524), ('F1C112', 74935296586.92188), ('F1C026', 73423889170.28864), ('F1C025', 71368641511.87292), ('F1C116', 71221025373.92935), ('F1C054', 65533462071.80786), ('F1C126', 64200156869.318115), ('F1B01', 61345768437.85024), ('F1C072', 57742141310.093506), ('F1C193', 57695920327.93494), ('F1A224', 51333613802.354004), ('F1C115', 48921204792.18335), ('F1C056', 46337193053.99744), ('F1C034', 44064353517.14575), ('F1A274', 43132268413.150635), ('F1B02', 40622136402.984375), ('F1C076', 38392290120.47089), ('F1C105', 38141369327.24243), ('F1C015', 37496705438.478516), ('F1C074', 37492120734.91455), ('F1C135', 36290279633.20447), ('F1B05', 29566051014.128784), ('F1B26', 28725445830.224854), ('F1C062', 28409754652.700264), ('F1C145', 26675800390.216797), ('F1C035', 24420586789.996338), ('F1C036', 23618842062.591675), ('F1C055', 22459934450.132202), ('F1A214', 22097606599.813477), ('F1C064', 19894815606.133698), ('F1C19DP', 18762481559.80664), ('F1C075', 18339310879.989014), ('F1C066', 18149352031.20337), ('F1C132', 16422607136.797302), ('F1C134', 16278248891.046143), ('F1C19OM', 12927162805.026123), ('F1B07', 12789889825.304565), ('F1C065', 11989725754.843018), ('F1C142', 11334532628.532806), ('F1B08', 9596067341.126526), ('F1B25', 8429297327.619415), ('F1B03', 6649502578.256439), ('F1C19IN', 5555877209.974365), ('F1C144', 5437274457.0513), ('F1C146', 3753059807.230835), ('F1C136', 3484805561.484192), ('F1C081', 2279683832.776965), ('F1D04', 11255.589294433594), ('F1D05', 1807.447525024414), ('F1TUFEFT', 4.798095703125), ('F1M04', 4.3616943359375), ('F1INSUFT', 4.0118408203125), ('F1LCAPFT', 3.1756591796875), ('F1GVGCFT', 2.806640625), ('F1C023', 2.3985595703125), ('F1PGGCFT', 2.2088623046875), ('F1COREXP', 1.807861328125), ('F1ENDMFT', 1.568359375), ('year', 1.467529296875), ('F1RSRCPC', 1.44091796875), ('F1H01', 1.3963623046875), ('F1STSVFT', 1.1614990234375), ('F1SAFBPC', 1.1591796875), ('F1ACSPPC', 0.9658203125), ('F1M06', 0.8580322265625), ('Rev_not_tuition', 0.78759765625), ('UNITID', 0.6810971671686216), ('F1OTRVFT', 0.66912841796875), ('F1C123', 0.65966796875), ('F1C073', 0.5050048828125), ('F1A344', 0.17626953125), ('F1OTEXFT', 0.126953125), ('F1M05', 0.090057373046875)]\n"
     ]
    }
   ],
   "source": [
    "#Feature Importance\n",
    "important_public = []\n",
    "for name, coef in zip(X_train_public_continuous.columns, lr_public['ols'].coef_):\n",
    "    if coef > 0:\n",
    "        important_public.append((name, coef))\n",
    "print(sort_tuple(important_public))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.idxmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            #if verbose:\n",
    "                #print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            #if verbose:\n",
    "                #print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    included.append('id')\n",
    "    print('resulting features:')\n",
    "    print(included)\n",
    "    \n",
    "    return included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-529-cb9ef49ad007>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstepwise_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_private_continuous\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_private_continuous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-495-aba68b6436e6>\u001b[0m in \u001b[0;36mstepwise_selection\u001b[1;34m(X, y, initial_list, threshold_in, threshold_out, verbose)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mchanged\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# forward step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mexcluded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mincluded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mnew_pval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexcluded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mnew_column\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexcluded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "stepwise_selection(X_train_private_continuous,y_train_private_continuous)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingDataError",
     "evalue": "exog contains inf or nans",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingDataError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-527-eaaf7db5e4af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstepwise_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_public_continuous\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_public_continuous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-495-aba68b6436e6>\u001b[0m in \u001b[0;36mstepwise_selection\u001b[1;34m(X, y, initial_list, threshold_in, threshold_out, verbose)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mnew_pval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexcluded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mnew_column\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexcluded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_constant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mincluded\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_column\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mnew_pval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_column\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_column\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mbest_pval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_pval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\statsmodels\\regression\\linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    890\u001b[0m                    \"An exception will be raised in the next version.\")\n\u001b[0;32m    891\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 892\u001b[1;33m         super(OLS, self).__init__(endog, exog, missing=missing,\n\u001b[0m\u001b[0;32m    893\u001b[0m                                   hasconst=hasconst, **kwargs)\n\u001b[0;32m    894\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"weights\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_keys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\statsmodels\\regression\\linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, weights, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    717\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m         super(WLS, self).__init__(endog, exog, missing=missing,\n\u001b[0m\u001b[0;32m    720\u001b[0m                                   weights=weights, hasconst=hasconst, **kwargs)\n\u001b[0;32m    721\u001b[0m         \u001b[0mnobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\statsmodels\\regression\\linear_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \"\"\"\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRegressionModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_attr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pinv_wexog'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wendog'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wexog'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'weights'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\statsmodels\\base\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\statsmodels\\base\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'missing'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'none'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mhasconst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hasconst'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         self.data = self._handle_data(endog, exog, missing, hasconst,\n\u001b[0m\u001b[0;32m     93\u001b[0m                                       **kwargs)\n\u001b[0;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_constant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\statsmodels\\base\\model.py\u001b[0m in \u001b[0;36m_handle_data\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_handle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhasconst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;31m# kwargs arrays could have changed, easier to just attach here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\statsmodels\\base\\data.py\u001b[0m in \u001b[0;36mhandle_data\u001b[1;34m(endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle_data_class_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m     return klass(endog, exog=exog, missing=missing, hasconst=hasconst,\n\u001b[0m\u001b[0;32m    674\u001b[0m                  **kwargs)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\statsmodels\\base\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconst_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk_constant\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_constant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhasconst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\statsmodels\\base\\data.py\u001b[0m in \u001b[0;36m_handle_constant\u001b[1;34m(self, hasconst)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mexog_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexog_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mMissingDataError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'exog contains inf or nans'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m             \u001b[0mexog_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[0mconst_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexog_max\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mexog_min\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMissingDataError\u001b[0m: exog contains inf or nans"
     ]
    }
   ],
   "source": [
    "stepwise_selection(X_train_public_continuous,y_train_public_continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>GBA6RTBK</td>     <th>  R-squared:         </th> <td>   0.449</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.424</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   18.65</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 01 Dec 2021</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:22:44</td>     <th>  Log-Likelihood:    </th> <td> -19802.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  4500</td>      <th>  AIC:               </th> <td>3.998e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  4311</td>      <th>  BIC:               </th> <td>4.119e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   188</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>-1.139e-05</td> <td> 5.52e-06</td> <td>   -2.065</td> <td> 0.039</td> <td>-2.22e-05</td> <td>-5.74e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> 5.208e-09</td> <td>  5.1e-09</td> <td>    1.021</td> <td> 0.307</td> <td> -4.8e-09</td> <td> 1.52e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0032</td> <td>    0.002</td> <td>    1.679</td> <td> 0.093</td> <td>   -0.001</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td> 9.901e-08</td> <td> 3.61e-08</td> <td>    2.746</td> <td> 0.006</td> <td> 2.83e-08</td> <td>  1.7e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -0.0004</td> <td>    0.000</td> <td>   -2.011</td> <td> 0.044</td> <td>   -0.001</td> <td>-1.12e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.0004</td> <td>    0.000</td> <td>    2.011</td> <td> 0.044</td> <td> 1.12e-05</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td> 9.431e-09</td> <td> 7.82e-09</td> <td>    1.205</td> <td> 0.228</td> <td>-5.91e-09</td> <td> 2.48e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>   -0.0005</td> <td>    0.000</td> <td>   -2.010</td> <td> 0.045</td> <td>   -0.001</td> <td>-1.31e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>   -0.0004</td> <td>    0.000</td> <td>   -2.009</td> <td> 0.045</td> <td>   -0.001</td> <td> -8.6e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   -0.0002</td> <td> 8.85e-05</td> <td>   -2.011</td> <td> 0.044</td> <td>   -0.000</td> <td> -4.5e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>   -0.0002</td> <td> 8.85e-05</td> <td>   -2.011</td> <td> 0.044</td> <td>   -0.000</td> <td>-4.51e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>   -0.0009</td> <td>    0.000</td> <td>   -2.011</td> <td> 0.044</td> <td>   -0.002</td> <td>-2.25e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>   -0.0005</td> <td>    0.000</td> <td>   -1.679</td> <td> 0.093</td> <td>   -0.001</td> <td> 9.06e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>   -0.0005</td> <td>    0.000</td> <td>   -1.679</td> <td> 0.093</td> <td>   -0.001</td> <td> 9.07e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>   -0.0005</td> <td>    0.000</td> <td>   -1.679</td> <td> 0.093</td> <td>   -0.001</td> <td> 9.06e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>   -0.0005</td> <td>    0.000</td> <td>   -1.679</td> <td> 0.093</td> <td>   -0.001</td> <td> 9.07e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>   -0.0005</td> <td>    0.000</td> <td>   -1.678</td> <td> 0.093</td> <td>   -0.001</td> <td> 9.07e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>   -0.0027</td> <td>    0.002</td> <td>   -1.679</td> <td> 0.093</td> <td>   -0.006</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    0.0032</td> <td>    0.002</td> <td>    1.679</td> <td> 0.093</td> <td>   -0.001</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>    0.0092</td> <td>    0.005</td> <td>    2.011</td> <td> 0.044</td> <td>    0.000</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>   -0.0092</td> <td>    0.005</td> <td>   -2.011</td> <td> 0.044</td> <td>   -0.018</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>    0.0092</td> <td>    0.005</td> <td>    2.011</td> <td> 0.044</td> <td>    0.000</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>   -0.0064</td> <td>    0.003</td> <td>   -2.011</td> <td> 0.044</td> <td>   -0.013</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>    0.0028</td> <td>    0.001</td> <td>    2.011</td> <td> 0.044</td> <td> 6.91e-05</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>    0.0028</td> <td>    0.001</td> <td>    2.011</td> <td> 0.044</td> <td> 6.91e-05</td> <td>    0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   -0.0009</td> <td>    0.000</td> <td>   -2.011</td> <td> 0.044</td> <td>   -0.002</td> <td>-2.23e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>-1.287e-07</td> <td> 2.53e-07</td> <td>   -0.509</td> <td> 0.611</td> <td>-6.24e-07</td> <td> 3.67e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>-1.723e-06</td> <td> 4.78e-07</td> <td>   -3.608</td> <td> 0.000</td> <td>-2.66e-06</td> <td>-7.87e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td> 6.725e-08</td> <td> 2.82e-07</td> <td>    0.239</td> <td> 0.811</td> <td>-4.85e-07</td> <td> 6.19e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td> 1.316e-06</td> <td> 1.41e-06</td> <td>    0.931</td> <td> 0.352</td> <td>-1.46e-06</td> <td> 4.09e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>  6.44e-08</td> <td> 2.46e-07</td> <td>    0.262</td> <td> 0.794</td> <td>-4.18e-07</td> <td> 5.47e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td> 7.002e-08</td> <td>  2.4e-07</td> <td>    0.292</td> <td> 0.771</td> <td>-4.01e-07</td> <td> 5.41e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>-6.104e-08</td> <td>  2.4e-07</td> <td>   -0.254</td> <td> 0.799</td> <td>-5.31e-07</td> <td> 4.09e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td> 1.562e-07</td> <td> 8.34e-08</td> <td>    1.874</td> <td> 0.061</td> <td>-7.24e-09</td> <td>  3.2e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>-3.132e-07</td> <td> 1.89e-07</td> <td>   -1.656</td> <td> 0.098</td> <td>-6.84e-07</td> <td> 5.76e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>-1.187e-08</td> <td> 5.16e-08</td> <td>   -0.230</td> <td> 0.818</td> <td>-1.13e-07</td> <td> 8.93e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>    0.0011</td> <td>    0.001</td> <td>    1.659</td> <td> 0.097</td> <td>   -0.000</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>    0.0006</td> <td>    0.001</td> <td>    0.814</td> <td> 0.415</td> <td>   -0.001</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>   -0.0003</td> <td>    0.001</td> <td>   -0.543</td> <td> 0.587</td> <td>   -0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>    0.0009</td> <td>    0.002</td> <td>    0.543</td> <td> 0.587</td> <td>   -0.002</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>    0.0009</td> <td>    0.000</td> <td>    2.014</td> <td> 0.044</td> <td> 2.27e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td>    0.0008</td> <td>    0.000</td> <td>    2.009</td> <td> 0.045</td> <td> 2.03e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td> 1.234e-05</td> <td> 7.98e-06</td> <td>    1.546</td> <td> 0.122</td> <td>-3.31e-06</td> <td>  2.8e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>-9.083e-09</td> <td> 1.02e-08</td> <td>   -0.891</td> <td> 0.373</td> <td>-2.91e-08</td> <td> 1.09e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>    0.0008</td> <td>    0.000</td> <td>    2.007</td> <td> 0.045</td> <td> 1.98e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>    0.0008</td> <td>    0.000</td> <td>    2.017</td> <td> 0.044</td> <td> 2.35e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td> 6.187e-06</td> <td> 6.19e-06</td> <td>    1.000</td> <td> 0.317</td> <td>-5.94e-06</td> <td> 1.83e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td> -8.09e-09</td> <td> 8.21e-09</td> <td>   -0.986</td> <td> 0.324</td> <td>-2.42e-08</td> <td>    8e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>    0.0009</td> <td>    0.000</td> <td>    2.029</td> <td> 0.042</td> <td> 2.92e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td>    0.0008</td> <td>    0.000</td> <td>    1.933</td> <td> 0.053</td> <td>-1.14e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td> 5.068e-05</td> <td> 3.48e-05</td> <td>    1.456</td> <td> 0.146</td> <td>-1.76e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td>-1.372e-08</td> <td> 1.25e-08</td> <td>   -1.101</td> <td> 0.271</td> <td>-3.82e-08</td> <td> 1.07e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td>    0.0008</td> <td>    0.000</td> <td>    1.899</td> <td> 0.058</td> <td>-2.65e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td>    0.0009</td> <td>    0.000</td> <td>    2.080</td> <td> 0.038</td> <td> 5.04e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>   <td> 4.039e-05</td> <td>  6.5e-05</td> <td>    0.622</td> <td> 0.534</td> <td> -8.7e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>   <td>   -0.0001</td> <td>    0.000</td> <td>   -0.521</td> <td> 0.602</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>   <td>    0.0008</td> <td>    0.000</td> <td>    1.900</td> <td> 0.058</td> <td>-2.62e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>   <td>    0.0009</td> <td>    0.000</td> <td>    2.078</td> <td> 0.038</td> <td> 4.95e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>   <td> 3.948e-05</td> <td>  6.5e-05</td> <td>    0.607</td> <td> 0.544</td> <td> -8.8e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>   <td>-9.943e-05</td> <td>    0.000</td> <td>   -0.510</td> <td> 0.610</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>   <td>    0.0008</td> <td>    0.000</td> <td>    1.924</td> <td> 0.054</td> <td> -1.6e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>   <td>    0.0009</td> <td>    0.000</td> <td>    2.033</td> <td> 0.042</td> <td> 3.07e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>   <td> 1.746e-05</td> <td> 7.89e-05</td> <td>    0.221</td> <td> 0.825</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>   <td>-4.864e-05</td> <td>    0.000</td> <td>   -0.206</td> <td> 0.837</td> <td>   -0.001</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>   <td>    0.0005</td> <td>    0.000</td> <td>    1.897</td> <td> 0.058</td> <td>-1.83e-05</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65</th>   <td>    0.0003</td> <td>    0.000</td> <td>    1.896</td> <td> 0.058</td> <td>-9.27e-06</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>   <td>    0.0003</td> <td>    0.000</td> <td>    1.898</td> <td> 0.058</td> <td>-8.97e-06</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>   <td>    0.0006</td> <td>    0.000</td> <td>    2.082</td> <td> 0.037</td> <td> 3.41e-05</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>   <td>    0.0003</td> <td>    0.000</td> <td>    2.083</td> <td> 0.037</td> <td> 1.72e-05</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>   <td>    0.0003</td> <td>    0.000</td> <td>    2.081</td> <td> 0.038</td> <td> 1.69e-05</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>   <td> 2.671e-05</td> <td> 4.33e-05</td> <td>    0.617</td> <td> 0.537</td> <td>-5.82e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>   <td> 1.346e-05</td> <td> 2.17e-05</td> <td>    0.622</td> <td> 0.534</td> <td> -2.9e-05</td> <td> 5.59e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>   <td> 1.323e-05</td> <td> 2.17e-05</td> <td>    0.611</td> <td> 0.541</td> <td>-2.92e-05</td> <td> 5.57e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>   <td>-6.853e-05</td> <td>    0.000</td> <td>   -0.527</td> <td> 0.598</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>   <td>-3.467e-05</td> <td>  6.5e-05</td> <td>   -0.534</td> <td> 0.594</td> <td>   -0.000</td> <td> 9.27e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>   <td>-3.387e-05</td> <td>  6.5e-05</td> <td>   -0.521</td> <td> 0.602</td> <td>   -0.000</td> <td> 9.35e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>   <td>    0.0008</td> <td>    0.000</td> <td>    1.896</td> <td> 0.058</td> <td>-2.76e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77</th>   <td>    0.0009</td> <td>    0.000</td> <td>    2.082</td> <td> 0.037</td> <td> 5.14e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>   <td> 4.219e-05</td> <td>  6.5e-05</td> <td>    0.649</td> <td> 0.516</td> <td>-8.52e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>   <td>   -0.0001</td> <td>    0.000</td> <td>   -0.541</td> <td> 0.588</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x80</th>   <td>    0.0008</td> <td>    0.000</td> <td>    1.897</td> <td> 0.058</td> <td>-2.75e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x81</th>   <td>    0.0009</td> <td>    0.000</td> <td>    2.082</td> <td> 0.037</td> <td> 5.13e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x82</th>   <td>  4.02e-05</td> <td>  6.5e-05</td> <td>    0.619</td> <td> 0.536</td> <td>-8.72e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x83</th>   <td>   -0.0001</td> <td>    0.000</td> <td>   -0.530</td> <td> 0.596</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x84</th>   <td>    0.0008</td> <td>    0.000</td> <td>    2.012</td> <td> 0.044</td> <td> 2.17e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x85</th>   <td>    0.0008</td> <td>    0.000</td> <td>    2.012</td> <td> 0.044</td> <td> 2.17e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x86</th>   <td>    0.0008</td> <td>    0.000</td> <td>    2.012</td> <td> 0.044</td> <td> 2.17e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x87</th>   <td>    0.0008</td> <td>    0.000</td> <td>    2.012</td> <td> 0.044</td> <td> 2.17e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x88</th>   <td>    0.0008</td> <td>    0.000</td> <td>    2.012</td> <td> 0.044</td> <td> 2.17e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x89</th>   <td>    0.0008</td> <td>    0.000</td> <td>    2.012</td> <td> 0.044</td> <td> 2.17e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x90</th>   <td>    0.0008</td> <td>    0.000</td> <td>    1.896</td> <td> 0.058</td> <td>-2.76e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x91</th>   <td>    0.0009</td> <td>    0.000</td> <td>    2.082</td> <td> 0.037</td> <td> 5.12e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x92</th>   <td> 3.396e-05</td> <td>  6.5e-05</td> <td>    0.522</td> <td> 0.601</td> <td>-9.35e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x93</th>   <td>-9.708e-05</td> <td>    0.000</td> <td>   -0.498</td> <td> 0.618</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x94</th>   <td>    0.0008</td> <td>    0.000</td> <td>    1.897</td> <td> 0.058</td> <td>-2.75e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x95</th>   <td>    0.0009</td> <td>    0.000</td> <td>    2.082</td> <td> 0.037</td> <td> 5.13e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x96</th>   <td> 4.021e-05</td> <td>  6.5e-05</td> <td>    0.619</td> <td> 0.536</td> <td>-8.71e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x97</th>   <td>   -0.0001</td> <td>    0.000</td> <td>   -0.530</td> <td> 0.596</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x98</th>   <td>   -0.0005</td> <td>    0.000</td> <td>   -1.852</td> <td> 0.064</td> <td>   -0.001</td> <td> 2.76e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x99</th>   <td>   -0.0007</td> <td>    0.000</td> <td>   -1.956</td> <td> 0.051</td> <td>   -0.001</td> <td> 1.56e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x100</th>  <td> 5.356e-05</td> <td> 6.27e-05</td> <td>    0.854</td> <td> 0.393</td> <td>-6.94e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x101</th>  <td>    0.0001</td> <td>    0.000</td> <td>    1.099</td> <td> 0.272</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x102</th>  <td> 2.971e-10</td> <td> 2.36e-10</td> <td>    1.261</td> <td> 0.208</td> <td>-1.65e-10</td> <td> 7.59e-10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x103</th>  <td> 8.973e-05</td> <td> 4.51e-05</td> <td>    1.992</td> <td> 0.046</td> <td>  1.4e-06</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x104</th>  <td> -2.08e-05</td> <td> 3.83e-05</td> <td>   -0.543</td> <td> 0.587</td> <td> -9.6e-05</td> <td> 5.44e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x105</th>  <td>-6.888e-05</td> <td> 5.74e-05</td> <td>   -1.201</td> <td> 0.230</td> <td>   -0.000</td> <td> 4.36e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x106</th>  <td>   -0.0005</td> <td>    0.000</td> <td>   -1.852</td> <td> 0.064</td> <td>   -0.001</td> <td> 2.75e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x107</th>  <td>-7.777e-05</td> <td> 7.69e-05</td> <td>   -1.011</td> <td> 0.312</td> <td>   -0.000</td> <td>  7.3e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x108</th>  <td> 3.276e-05</td> <td> 4.35e-05</td> <td>    0.754</td> <td> 0.451</td> <td>-5.24e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x109</th>  <td> 8.041e-05</td> <td> 8.56e-05</td> <td>    0.939</td> <td> 0.348</td> <td>-8.75e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x110</th>  <td>    0.0010</td> <td>    0.000</td> <td>    2.007</td> <td> 0.045</td> <td> 2.23e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x111</th>  <td>-6.892e-07</td> <td> 4.16e-07</td> <td>   -1.656</td> <td> 0.098</td> <td>-1.51e-06</td> <td> 1.27e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x112</th>  <td>    0.0010</td> <td>    0.000</td> <td>    2.006</td> <td> 0.045</td> <td> 2.19e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x113</th>  <td>-3.571e-08</td> <td> 4.45e-07</td> <td>   -0.080</td> <td> 0.936</td> <td>-9.08e-07</td> <td> 8.36e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x114</th>  <td>    0.0010</td> <td>    0.000</td> <td>    2.007</td> <td> 0.045</td> <td> 2.21e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x115</th>  <td>-5.003e-07</td> <td>  6.9e-07</td> <td>   -0.725</td> <td> 0.468</td> <td>-1.85e-06</td> <td> 8.52e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x116</th>  <td>    0.0010</td> <td>    0.000</td> <td>    2.007</td> <td> 0.045</td> <td> 2.23e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x117</th>  <td>-6.875e-07</td> <td> 4.32e-07</td> <td>   -1.591</td> <td> 0.112</td> <td>-1.53e-06</td> <td>  1.6e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x118</th>  <td>    0.0010</td> <td>    0.000</td> <td>    2.007</td> <td> 0.045</td> <td> 2.24e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x119</th>  <td> 2.163e-07</td> <td> 4.89e-07</td> <td>    0.443</td> <td> 0.658</td> <td>-7.42e-07</td> <td> 1.17e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x120</th>  <td>    0.0010</td> <td>    0.000</td> <td>    2.007</td> <td> 0.045</td> <td> 2.24e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x121</th>  <td>-8.654e-07</td> <td> 4.26e-07</td> <td>   -2.031</td> <td> 0.042</td> <td> -1.7e-06</td> <td>-3.02e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x122</th>  <td>    0.0010</td> <td>    0.000</td> <td>    2.007</td> <td> 0.045</td> <td> 2.25e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x123</th>  <td> -9.83e-07</td> <td> 4.36e-07</td> <td>   -2.256</td> <td> 0.024</td> <td>-1.84e-06</td> <td>-1.29e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x124</th>  <td>    0.0010</td> <td>    0.000</td> <td>    2.006</td> <td> 0.045</td> <td>  2.2e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x125</th>  <td>    0.0010</td> <td>    0.000</td> <td>    2.007</td> <td> 0.045</td> <td> 2.24e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x126</th>  <td>-6.445e-07</td> <td> 4.39e-07</td> <td>   -1.468</td> <td> 0.142</td> <td>-1.51e-06</td> <td> 2.16e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x127</th>  <td>    0.0010</td> <td>    0.000</td> <td>    2.007</td> <td> 0.045</td> <td> 2.24e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x128</th>  <td>-4.159e-07</td> <td> 4.28e-07</td> <td>   -0.971</td> <td> 0.331</td> <td>-1.26e-06</td> <td> 4.24e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x129</th>  <td>    0.0010</td> <td>    0.000</td> <td>    2.007</td> <td> 0.045</td> <td> 2.24e-05</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x130</th>  <td>-7.063e-07</td> <td>  4.4e-07</td> <td>   -1.604</td> <td> 0.109</td> <td>-1.57e-06</td> <td> 1.57e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x131</th>  <td>   -0.0008</td> <td>    0.000</td> <td>   -2.007</td> <td> 0.045</td> <td>   -0.002</td> <td> -1.9e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x132</th>  <td>   -0.0001</td> <td> 6.84e-05</td> <td>   -1.998</td> <td> 0.046</td> <td>   -0.000</td> <td>-2.59e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x133</th>  <td>   -0.0001</td> <td> 6.84e-05</td> <td>   -2.006</td> <td> 0.045</td> <td>   -0.000</td> <td>-3.09e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x134</th>  <td>   -0.0001</td> <td> 6.84e-05</td> <td>   -2.008</td> <td> 0.045</td> <td>   -0.000</td> <td>-3.23e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x135</th>  <td>   -0.0001</td> <td> 6.84e-05</td> <td>   -2.018</td> <td> 0.044</td> <td>   -0.000</td> <td>-3.96e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x136</th>  <td>   -0.0001</td> <td> 6.84e-05</td> <td>   -2.005</td> <td> 0.045</td> <td>   -0.000</td> <td>-3.04e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x137</th>  <td>   -0.0001</td> <td> 6.84e-05</td> <td>   -2.006</td> <td> 0.045</td> <td>   -0.000</td> <td>-3.12e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x138</th>  <td>    2.5772</td> <td>    2.509</td> <td>    1.027</td> <td> 0.304</td> <td>   -2.342</td> <td>    7.496</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x139</th>  <td> 2.589e-09</td> <td> 1.32e-08</td> <td>    0.196</td> <td> 0.845</td> <td>-2.33e-08</td> <td> 2.85e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x140</th>  <td>-1.236e-08</td> <td> 1.17e-08</td> <td>   -1.059</td> <td> 0.290</td> <td>-3.52e-08</td> <td> 1.05e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x141</th>  <td>    0.0046</td> <td>    0.037</td> <td>    0.124</td> <td> 0.901</td> <td>   -0.067</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x142</th>  <td>-4.494e-08</td> <td> 1.31e-08</td> <td>   -3.440</td> <td> 0.001</td> <td>-7.06e-08</td> <td>-1.93e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x143</th>  <td>    0.5603</td> <td>    0.539</td> <td>    1.039</td> <td> 0.299</td> <td>   -0.497</td> <td>    1.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x144</th>  <td>    0.6934</td> <td>    0.544</td> <td>    1.274</td> <td> 0.203</td> <td>   -0.374</td> <td>    1.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x145</th>  <td>    0.5697</td> <td>    0.540</td> <td>    1.055</td> <td> 0.292</td> <td>   -0.489</td> <td>    1.629</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x146</th>  <td>    0.6108</td> <td>    0.540</td> <td>    1.131</td> <td> 0.258</td> <td>   -0.448</td> <td>    1.669</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x147</th>  <td>    0.6295</td> <td>    0.541</td> <td>    1.163</td> <td> 0.245</td> <td>   -0.431</td> <td>    1.690</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x148</th>  <td>    0.0007</td> <td>    0.000</td> <td>    5.831</td> <td> 0.000</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x149</th>  <td>   -0.0013</td> <td>    0.000</td> <td>   -3.084</td> <td> 0.002</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x150</th>  <td>    0.0001</td> <td> 8.77e-05</td> <td>    1.155</td> <td> 0.248</td> <td>-7.06e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x151</th>  <td> -9.45e-05</td> <td> 4.39e-05</td> <td>   -2.154</td> <td> 0.031</td> <td>   -0.000</td> <td>-8.48e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x152</th>  <td>   -0.0004</td> <td>    0.000</td> <td>   -2.913</td> <td> 0.004</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x153</th>  <td> 3.813e-08</td> <td>  1.5e-08</td> <td>    2.546</td> <td> 0.011</td> <td> 8.77e-09</td> <td> 6.75e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x154</th>  <td>   -0.3467</td> <td>    0.518</td> <td>   -0.669</td> <td> 0.503</td> <td>   -1.362</td> <td>    0.669</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x155</th>  <td>   -1.0078</td> <td>    0.561</td> <td>   -1.795</td> <td> 0.073</td> <td>   -2.108</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x156</th>  <td>   -1.1416</td> <td>    0.618</td> <td>   -1.846</td> <td> 0.065</td> <td>   -2.354</td> <td>    0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x157</th>  <td>   -0.3176</td> <td>    0.523</td> <td>   -0.607</td> <td> 0.544</td> <td>   -1.344</td> <td>    0.709</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x158</th>  <td>   -0.9774</td> <td>    0.523</td> <td>   -1.869</td> <td> 0.062</td> <td>   -2.002</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x159</th>  <td>   -0.6344</td> <td>    0.519</td> <td>   -1.222</td> <td> 0.222</td> <td>   -1.652</td> <td>    0.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x160</th>  <td>   -0.2460</td> <td>    0.523</td> <td>   -0.470</td> <td> 0.638</td> <td>   -1.271</td> <td>    0.779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x161</th>  <td>    0.0003</td> <td>    0.000</td> <td>    1.475</td> <td> 0.140</td> <td>-9.27e-05</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x162</th>  <td>    0.0020</td> <td>    0.001</td> <td>    3.322</td> <td> 0.001</td> <td>    0.001</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x163</th>  <td>    0.0033</td> <td>    0.001</td> <td>    2.845</td> <td> 0.004</td> <td>    0.001</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x164</th>  <td>    0.0004</td> <td>    0.000</td> <td>    1.421</td> <td> 0.155</td> <td>   -0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x165</th>  <td>    0.0012</td> <td>    0.000</td> <td>    3.040</td> <td> 0.002</td> <td>    0.000</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x166</th>  <td>    0.0002</td> <td>    0.000</td> <td>    0.828</td> <td> 0.408</td> <td>   -0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x167</th>  <td>   -0.0008</td> <td>    0.000</td> <td>   -2.790</td> <td> 0.005</td> <td>   -0.001</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x168</th>  <td>    0.1548</td> <td>    0.125</td> <td>    1.241</td> <td> 0.215</td> <td>   -0.090</td> <td>    0.399</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x169</th>  <td>   -0.1908</td> <td>    0.150</td> <td>   -1.273</td> <td> 0.203</td> <td>   -0.485</td> <td>    0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x170</th>  <td> 1.926e-05</td> <td> 5.52e-06</td> <td>    3.492</td> <td> 0.000</td> <td> 8.45e-06</td> <td> 3.01e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x171</th>  <td>    0.0558</td> <td>    0.021</td> <td>    2.599</td> <td> 0.009</td> <td>    0.014</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x172</th>  <td>-3.891e-07</td> <td> 2.44e-07</td> <td>   -1.597</td> <td> 0.110</td> <td>-8.67e-07</td> <td> 8.85e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x173</th>  <td> 3.601e-07</td> <td> 8.16e-07</td> <td>    0.441</td> <td> 0.659</td> <td>-1.24e-06</td> <td> 1.96e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x174</th>  <td> 9.141e-07</td> <td> 5.44e-07</td> <td>    1.682</td> <td> 0.093</td> <td>-1.52e-07</td> <td> 1.98e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x175</th>  <td>-2.953e-07</td> <td> 7.24e-07</td> <td>   -0.408</td> <td> 0.684</td> <td>-1.72e-06</td> <td> 1.12e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x176</th>  <td>-1.448e-08</td> <td> 8.97e-08</td> <td>   -0.161</td> <td> 0.872</td> <td> -1.9e-07</td> <td> 1.61e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x177</th>  <td> 5.481e-07</td> <td> 6.32e-07</td> <td>    0.867</td> <td> 0.386</td> <td>-6.92e-07</td> <td> 1.79e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x178</th>  <td> 3.567e-07</td> <td> 1.06e-06</td> <td>    0.337</td> <td> 0.736</td> <td>-1.72e-06</td> <td> 2.43e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x179</th>  <td> 1.128e-06</td> <td> 1.37e-06</td> <td>    0.821</td> <td> 0.412</td> <td>-1.57e-06</td> <td> 3.82e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x180</th>  <td>-6.818e-07</td> <td> 1.81e-06</td> <td>   -0.376</td> <td> 0.707</td> <td>-4.23e-06</td> <td> 2.87e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x181</th>  <td> -1.06e-08</td> <td> 1.46e-07</td> <td>   -0.072</td> <td> 0.942</td> <td>-2.97e-07</td> <td> 2.76e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x182</th>  <td>  -1.6e-06</td> <td> 1.31e-06</td> <td>   -1.220</td> <td> 0.222</td> <td>-4.17e-06</td> <td> 9.71e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x183</th>  <td> 2.451e-06</td> <td> 4.73e-06</td> <td>    0.518</td> <td> 0.604</td> <td>-6.82e-06</td> <td> 1.17e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x184</th>  <td> 2.264e-06</td> <td> 6.62e-06</td> <td>    0.342</td> <td> 0.732</td> <td>-1.07e-05</td> <td> 1.52e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x185</th>  <td> 4.714e-07</td> <td> 8.48e-06</td> <td>    0.056</td> <td> 0.956</td> <td>-1.61e-05</td> <td> 1.71e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x186</th>  <td>-5.587e-08</td> <td> 4.85e-07</td> <td>   -0.115</td> <td> 0.908</td> <td>-1.01e-06</td> <td> 8.94e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x187</th>  <td>-5.168e-07</td> <td> 5.06e-07</td> <td>   -1.022</td> <td> 0.307</td> <td>-1.51e-06</td> <td> 4.74e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x188</th>  <td>-9.773e-07</td> <td> 1.03e-06</td> <td>   -0.950</td> <td> 0.342</td> <td>-2.99e-06</td> <td> 1.04e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x189</th>  <td> 1.772e-06</td> <td> 8.17e-07</td> <td>    2.169</td> <td> 0.030</td> <td>  1.7e-07</td> <td> 3.37e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x190</th>  <td> 5.231e-07</td> <td> 1.53e-06</td> <td>    0.343</td> <td> 0.732</td> <td>-2.47e-06</td> <td> 3.52e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x191</th>  <td> 2.714e-07</td> <td> 1.55e-07</td> <td>    1.752</td> <td> 0.080</td> <td>-3.23e-08</td> <td> 5.75e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x192</th>  <td>-1.201e-06</td> <td> 9.77e-07</td> <td>   -1.230</td> <td> 0.219</td> <td>-3.12e-06</td> <td> 7.13e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x193</th>  <td>  1.37e-06</td> <td>    1e-06</td> <td>    1.364</td> <td> 0.173</td> <td>-5.99e-07</td> <td> 3.34e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x194</th>  <td>-1.446e-06</td> <td> 1.16e-06</td> <td>   -1.252</td> <td> 0.211</td> <td>-3.71e-06</td> <td> 8.19e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x195</th>  <td> 1.262e-06</td> <td> 1.62e-06</td> <td>    0.779</td> <td> 0.436</td> <td>-1.92e-06</td> <td> 4.44e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x196</th>  <td> 4.902e-07</td> <td>  1.5e-07</td> <td>    3.269</td> <td> 0.001</td> <td> 1.96e-07</td> <td> 7.84e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x197</th>  <td>-2.985e-08</td> <td>  3.7e-07</td> <td>   -0.081</td> <td> 0.936</td> <td>-7.56e-07</td> <td> 6.96e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x198</th>  <td> 1.138e-06</td> <td>  7.4e-07</td> <td>    1.539</td> <td> 0.124</td> <td>-3.12e-07</td> <td> 2.59e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x199</th>  <td> 3.721e-08</td> <td> 7.66e-07</td> <td>    0.049</td> <td> 0.961</td> <td>-1.47e-06</td> <td> 1.54e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x200</th>  <td>-1.228e-08</td> <td> 7.17e-07</td> <td>   -0.017</td> <td> 0.986</td> <td>-1.42e-06</td> <td> 1.39e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x201</th>  <td>-5.413e-08</td> <td> 1.43e-07</td> <td>   -0.379</td> <td> 0.705</td> <td>-3.34e-07</td> <td> 2.26e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x202</th>  <td>-2.697e-07</td> <td> 5.98e-07</td> <td>   -0.451</td> <td> 0.652</td> <td>-1.44e-06</td> <td> 9.03e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x203</th>  <td>  9.23e-07</td> <td> 7.11e-07</td> <td>    1.298</td> <td> 0.194</td> <td>-4.71e-07</td> <td> 2.32e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x204</th>  <td> 1.489e-07</td> <td> 5.21e-07</td> <td>    0.286</td> <td> 0.775</td> <td>-8.72e-07</td> <td> 1.17e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x205</th>  <td> 1.111e-08</td> <td> 6.45e-07</td> <td>    0.017</td> <td> 0.986</td> <td>-1.25e-06</td> <td> 1.28e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x206</th>  <td> 3.158e-07</td> <td> 1.87e-07</td> <td>    1.688</td> <td> 0.091</td> <td>-5.09e-08</td> <td> 6.82e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x207</th>  <td>  3.42e-08</td> <td>  3.6e-07</td> <td>    0.095</td> <td> 0.924</td> <td>-6.71e-07</td> <td>  7.4e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x208</th>  <td> 8.613e-07</td> <td> 5.79e-07</td> <td>    1.487</td> <td> 0.137</td> <td>-2.74e-07</td> <td>    2e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x209</th>  <td>-6.485e-07</td> <td> 9.87e-07</td> <td>   -0.657</td> <td> 0.511</td> <td>-2.58e-06</td> <td> 1.29e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x210</th>  <td> 1.149e-06</td> <td>  9.8e-07</td> <td>    1.171</td> <td> 0.242</td> <td>-7.74e-07</td> <td> 3.07e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x211</th>  <td>  1.76e-06</td> <td> 1.91e-06</td> <td>    0.920</td> <td> 0.358</td> <td>-1.99e-06</td> <td> 5.51e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x212</th>  <td> -3.99e-07</td> <td> 1.66e-07</td> <td>   -2.402</td> <td> 0.016</td> <td>-7.25e-07</td> <td>-7.33e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x213</th>  <td>-9.814e-07</td> <td> 6.99e-07</td> <td>   -1.405</td> <td> 0.160</td> <td>-2.35e-06</td> <td> 3.88e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x214</th>  <td>-6.133e-06</td> <td> 4.05e-06</td> <td>   -1.513</td> <td> 0.130</td> <td>-1.41e-05</td> <td> 1.81e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x215</th>  <td> 5.568e-06</td> <td> 2.55e-06</td> <td>    2.183</td> <td> 0.029</td> <td> 5.68e-07</td> <td> 1.06e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x216</th>  <td>-5.652e-06</td> <td> 6.53e-06</td> <td>   -0.866</td> <td> 0.387</td> <td>-1.84e-05</td> <td> 7.14e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x217</th>  <td> -2.54e-08</td> <td> 2.11e-08</td> <td>   -1.206</td> <td> 0.228</td> <td>-6.67e-08</td> <td> 1.59e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x218</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x219</th>  <td>-2.148e-06</td> <td> 1.07e-06</td> <td>   -2.006</td> <td> 0.045</td> <td>-4.25e-06</td> <td>-4.91e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x220</th>  <td> 3.407e-06</td> <td> 1.89e-06</td> <td>    1.806</td> <td> 0.071</td> <td>-2.91e-07</td> <td>  7.1e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x221</th>  <td> 4.124e-07</td> <td> 5.97e-07</td> <td>    0.691</td> <td> 0.490</td> <td>-7.58e-07</td> <td> 1.58e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x222</th>  <td> 8.338e-07</td> <td> 8.32e-07</td> <td>    1.002</td> <td> 0.316</td> <td>-7.97e-07</td> <td> 2.47e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x223</th>  <td>-1.792e-06</td> <td> 1.12e-06</td> <td>   -1.603</td> <td> 0.109</td> <td>-3.98e-06</td> <td> 3.99e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x224</th>  <td>-6.512e-07</td> <td> 2.93e-07</td> <td>   -2.220</td> <td> 0.026</td> <td>-1.23e-06</td> <td>-7.62e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x225</th>  <td> 4.453e-06</td> <td> 1.25e-06</td> <td>    3.576</td> <td> 0.000</td> <td> 2.01e-06</td> <td> 6.89e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x226</th>  <td> 8.512e-07</td> <td> 1.62e-06</td> <td>    0.526</td> <td> 0.599</td> <td>-2.32e-06</td> <td> 4.03e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x227</th>  <td>-1.839e-05</td> <td> 7.19e-06</td> <td>   -2.557</td> <td> 0.011</td> <td>-3.25e-05</td> <td>-4.29e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x228</th>  <td> 2.387e-05</td> <td> 9.29e-06</td> <td>    2.569</td> <td> 0.010</td> <td> 5.65e-06</td> <td> 4.21e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x229</th>  <td>-2.726e-07</td> <td> 1.88e-07</td> <td>   -1.447</td> <td> 0.148</td> <td>-6.42e-07</td> <td> 9.67e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x230</th>  <td>-2.002e-10</td> <td> 2.51e-10</td> <td>   -0.798</td> <td> 0.425</td> <td>-6.92e-10</td> <td> 2.92e-10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x231</th>  <td>-5.607e-08</td> <td> 2.59e-08</td> <td>   -2.169</td> <td> 0.030</td> <td>-1.07e-07</td> <td>-5.38e-09</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>394.001</td> <th>  Durbin-Watson:     </th> <td>   1.980</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 696.174</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.620</td>  <th>  Prob(JB):          </th> <td>6.73e-152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.475</td>  <th>  Cond. No.          </th> <td>1.08e+16</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.08e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:               GBA6RTBK   R-squared:                       0.449\n",
       "Model:                            OLS   Adj. R-squared:                  0.424\n",
       "Method:                 Least Squares   F-statistic:                     18.65\n",
       "Date:                Wed, 01 Dec 2021   Prob (F-statistic):               0.00\n",
       "Time:                        21:22:44   Log-Likelihood:                -19802.\n",
       "No. Observations:                4500   AIC:                         3.998e+04\n",
       "Df Residuals:                    4311   BIC:                         4.119e+04\n",
       "Df Model:                         188                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1         -1.139e-05   5.52e-06     -2.065      0.039   -2.22e-05   -5.74e-07\n",
       "x2          5.208e-09    5.1e-09      1.021      0.307    -4.8e-09    1.52e-08\n",
       "x3             0.0032      0.002      1.679      0.093      -0.001       0.007\n",
       "x4          9.901e-08   3.61e-08      2.746      0.006    2.83e-08     1.7e-07\n",
       "x5            -0.0004      0.000     -2.011      0.044      -0.001   -1.12e-05\n",
       "x6             0.0004      0.000      2.011      0.044    1.12e-05       0.001\n",
       "x7          9.431e-09   7.82e-09      1.205      0.228   -5.91e-09    2.48e-08\n",
       "x8            -0.0005      0.000     -2.010      0.045      -0.001   -1.31e-05\n",
       "x9            -0.0004      0.000     -2.009      0.045      -0.001    -8.6e-06\n",
       "x10           -0.0002   8.85e-05     -2.011      0.044      -0.000    -4.5e-06\n",
       "x11           -0.0002   8.85e-05     -2.011      0.044      -0.000   -4.51e-06\n",
       "x12           -0.0009      0.000     -2.011      0.044      -0.002   -2.25e-05\n",
       "x13           -0.0005      0.000     -1.679      0.093      -0.001    9.06e-05\n",
       "x14           -0.0005      0.000     -1.679      0.093      -0.001    9.07e-05\n",
       "x15           -0.0005      0.000     -1.679      0.093      -0.001    9.06e-05\n",
       "x16           -0.0005      0.000     -1.679      0.093      -0.001    9.07e-05\n",
       "x17           -0.0005      0.000     -1.678      0.093      -0.001    9.07e-05\n",
       "x18           -0.0027      0.002     -1.679      0.093      -0.006       0.000\n",
       "x19            0.0032      0.002      1.679      0.093      -0.001       0.007\n",
       "x20            0.0092      0.005      2.011      0.044       0.000       0.018\n",
       "x21           -0.0092      0.005     -2.011      0.044      -0.018      -0.000\n",
       "x22            0.0092      0.005      2.011      0.044       0.000       0.018\n",
       "x23           -0.0064      0.003     -2.011      0.044      -0.013      -0.000\n",
       "x24            0.0028      0.001      2.011      0.044    6.91e-05       0.005\n",
       "x25            0.0028      0.001      2.011      0.044    6.91e-05       0.005\n",
       "x26           -0.0009      0.000     -2.011      0.044      -0.002   -2.23e-05\n",
       "x27        -1.287e-07   2.53e-07     -0.509      0.611   -6.24e-07    3.67e-07\n",
       "x28        -1.723e-06   4.78e-07     -3.608      0.000   -2.66e-06   -7.87e-07\n",
       "x29         6.725e-08   2.82e-07      0.239      0.811   -4.85e-07    6.19e-07\n",
       "x30         1.316e-06   1.41e-06      0.931      0.352   -1.46e-06    4.09e-06\n",
       "x31          6.44e-08   2.46e-07      0.262      0.794   -4.18e-07    5.47e-07\n",
       "x32         7.002e-08    2.4e-07      0.292      0.771   -4.01e-07    5.41e-07\n",
       "x33        -6.104e-08    2.4e-07     -0.254      0.799   -5.31e-07    4.09e-07\n",
       "x34         1.562e-07   8.34e-08      1.874      0.061   -7.24e-09     3.2e-07\n",
       "x35        -3.132e-07   1.89e-07     -1.656      0.098   -6.84e-07    5.76e-08\n",
       "x36        -1.187e-08   5.16e-08     -0.230      0.818   -1.13e-07    8.93e-08\n",
       "x37            0.0011      0.001      1.659      0.097      -0.000       0.002\n",
       "x38            0.0006      0.001      0.814      0.415      -0.001       0.002\n",
       "x39           -0.0003      0.001     -0.543      0.587      -0.001       0.001\n",
       "x40            0.0009      0.002      0.543      0.587      -0.002       0.004\n",
       "x41            0.0009      0.000      2.014      0.044    2.27e-05       0.002\n",
       "x42            0.0008      0.000      2.009      0.045    2.03e-05       0.002\n",
       "x43         1.234e-05   7.98e-06      1.546      0.122   -3.31e-06     2.8e-05\n",
       "const      -9.083e-09   1.02e-08     -0.891      0.373   -2.91e-08    1.09e-08\n",
       "x44            0.0008      0.000      2.007      0.045    1.98e-05       0.002\n",
       "x45            0.0008      0.000      2.017      0.044    2.35e-05       0.002\n",
       "x46         6.187e-06   6.19e-06      1.000      0.317   -5.94e-06    1.83e-05\n",
       "x47         -8.09e-09   8.21e-09     -0.986      0.324   -2.42e-08       8e-09\n",
       "x48            0.0009      0.000      2.029      0.042    2.92e-05       0.002\n",
       "x49            0.0008      0.000      1.933      0.053   -1.14e-05       0.002\n",
       "x50         5.068e-05   3.48e-05      1.456      0.146   -1.76e-05       0.000\n",
       "x51        -1.372e-08   1.25e-08     -1.101      0.271   -3.82e-08    1.07e-08\n",
       "x52            0.0008      0.000      1.899      0.058   -2.65e-05       0.002\n",
       "x53            0.0009      0.000      2.080      0.038    5.04e-05       0.002\n",
       "x54         4.039e-05    6.5e-05      0.622      0.534    -8.7e-05       0.000\n",
       "x55           -0.0001      0.000     -0.521      0.602      -0.000       0.000\n",
       "x56            0.0008      0.000      1.900      0.058   -2.62e-05       0.002\n",
       "x57            0.0009      0.000      2.078      0.038    4.95e-05       0.002\n",
       "x58         3.948e-05    6.5e-05      0.607      0.544    -8.8e-05       0.000\n",
       "x59        -9.943e-05      0.000     -0.510      0.610      -0.000       0.000\n",
       "x60            0.0008      0.000      1.924      0.054    -1.6e-05       0.002\n",
       "x61            0.0009      0.000      2.033      0.042    3.07e-05       0.002\n",
       "x62         1.746e-05   7.89e-05      0.221      0.825      -0.000       0.000\n",
       "x63        -4.864e-05      0.000     -0.206      0.837      -0.001       0.000\n",
       "x64            0.0005      0.000      1.897      0.058   -1.83e-05       0.001\n",
       "x65            0.0003      0.000      1.896      0.058   -9.27e-06       0.001\n",
       "x66            0.0003      0.000      1.898      0.058   -8.97e-06       0.001\n",
       "x67            0.0006      0.000      2.082      0.037    3.41e-05       0.001\n",
       "x68            0.0003      0.000      2.083      0.037    1.72e-05       0.001\n",
       "x69            0.0003      0.000      2.081      0.038    1.69e-05       0.001\n",
       "x70         2.671e-05   4.33e-05      0.617      0.537   -5.82e-05       0.000\n",
       "x71         1.346e-05   2.17e-05      0.622      0.534    -2.9e-05    5.59e-05\n",
       "x72         1.323e-05   2.17e-05      0.611      0.541   -2.92e-05    5.57e-05\n",
       "x73        -6.853e-05      0.000     -0.527      0.598      -0.000       0.000\n",
       "x74        -3.467e-05    6.5e-05     -0.534      0.594      -0.000    9.27e-05\n",
       "x75        -3.387e-05    6.5e-05     -0.521      0.602      -0.000    9.35e-05\n",
       "x76            0.0008      0.000      1.896      0.058   -2.76e-05       0.002\n",
       "x77            0.0009      0.000      2.082      0.037    5.14e-05       0.002\n",
       "x78         4.219e-05    6.5e-05      0.649      0.516   -8.52e-05       0.000\n",
       "x79           -0.0001      0.000     -0.541      0.588      -0.000       0.000\n",
       "x80            0.0008      0.000      1.897      0.058   -2.75e-05       0.002\n",
       "x81            0.0009      0.000      2.082      0.037    5.13e-05       0.002\n",
       "x82          4.02e-05    6.5e-05      0.619      0.536   -8.72e-05       0.000\n",
       "x83           -0.0001      0.000     -0.530      0.596      -0.000       0.000\n",
       "x84            0.0008      0.000      2.012      0.044    2.17e-05       0.002\n",
       "x85            0.0008      0.000      2.012      0.044    2.17e-05       0.002\n",
       "x86            0.0008      0.000      2.012      0.044    2.17e-05       0.002\n",
       "x87            0.0008      0.000      2.012      0.044    2.17e-05       0.002\n",
       "x88            0.0008      0.000      2.012      0.044    2.17e-05       0.002\n",
       "x89            0.0008      0.000      2.012      0.044    2.17e-05       0.002\n",
       "x90            0.0008      0.000      1.896      0.058   -2.76e-05       0.002\n",
       "x91            0.0009      0.000      2.082      0.037    5.12e-05       0.002\n",
       "x92         3.396e-05    6.5e-05      0.522      0.601   -9.35e-05       0.000\n",
       "x93        -9.708e-05      0.000     -0.498      0.618      -0.000       0.000\n",
       "x94            0.0008      0.000      1.897      0.058   -2.75e-05       0.002\n",
       "x95            0.0009      0.000      2.082      0.037    5.13e-05       0.002\n",
       "x96         4.021e-05    6.5e-05      0.619      0.536   -8.71e-05       0.000\n",
       "x97           -0.0001      0.000     -0.530      0.596      -0.000       0.000\n",
       "x98           -0.0005      0.000     -1.852      0.064      -0.001    2.76e-05\n",
       "x99           -0.0007      0.000     -1.956      0.051      -0.001    1.56e-06\n",
       "x100        5.356e-05   6.27e-05      0.854      0.393   -6.94e-05       0.000\n",
       "x101           0.0001      0.000      1.099      0.272      -0.000       0.000\n",
       "x102        2.971e-10   2.36e-10      1.261      0.208   -1.65e-10    7.59e-10\n",
       "x103        8.973e-05   4.51e-05      1.992      0.046     1.4e-06       0.000\n",
       "x104        -2.08e-05   3.83e-05     -0.543      0.587    -9.6e-05    5.44e-05\n",
       "x105       -6.888e-05   5.74e-05     -1.201      0.230      -0.000    4.36e-05\n",
       "x106          -0.0005      0.000     -1.852      0.064      -0.001    2.75e-05\n",
       "x107       -7.777e-05   7.69e-05     -1.011      0.312      -0.000     7.3e-05\n",
       "x108        3.276e-05   4.35e-05      0.754      0.451   -5.24e-05       0.000\n",
       "x109        8.041e-05   8.56e-05      0.939      0.348   -8.75e-05       0.000\n",
       "x110           0.0010      0.000      2.007      0.045    2.23e-05       0.002\n",
       "x111       -6.892e-07   4.16e-07     -1.656      0.098   -1.51e-06    1.27e-07\n",
       "x112           0.0010      0.000      2.006      0.045    2.19e-05       0.002\n",
       "x113       -3.571e-08   4.45e-07     -0.080      0.936   -9.08e-07    8.36e-07\n",
       "x114           0.0010      0.000      2.007      0.045    2.21e-05       0.002\n",
       "x115       -5.003e-07    6.9e-07     -0.725      0.468   -1.85e-06    8.52e-07\n",
       "x116           0.0010      0.000      2.007      0.045    2.23e-05       0.002\n",
       "x117       -6.875e-07   4.32e-07     -1.591      0.112   -1.53e-06     1.6e-07\n",
       "x118           0.0010      0.000      2.007      0.045    2.24e-05       0.002\n",
       "x119        2.163e-07   4.89e-07      0.443      0.658   -7.42e-07    1.17e-06\n",
       "x120           0.0010      0.000      2.007      0.045    2.24e-05       0.002\n",
       "x121       -8.654e-07   4.26e-07     -2.031      0.042    -1.7e-06   -3.02e-08\n",
       "x122           0.0010      0.000      2.007      0.045    2.25e-05       0.002\n",
       "x123        -9.83e-07   4.36e-07     -2.256      0.024   -1.84e-06   -1.29e-07\n",
       "x124           0.0010      0.000      2.006      0.045     2.2e-05       0.002\n",
       "x125           0.0010      0.000      2.007      0.045    2.24e-05       0.002\n",
       "x126       -6.445e-07   4.39e-07     -1.468      0.142   -1.51e-06    2.16e-07\n",
       "x127           0.0010      0.000      2.007      0.045    2.24e-05       0.002\n",
       "x128       -4.159e-07   4.28e-07     -0.971      0.331   -1.26e-06    4.24e-07\n",
       "x129           0.0010      0.000      2.007      0.045    2.24e-05       0.002\n",
       "x130       -7.063e-07    4.4e-07     -1.604      0.109   -1.57e-06    1.57e-07\n",
       "x131          -0.0008      0.000     -2.007      0.045      -0.002    -1.9e-05\n",
       "x132          -0.0001   6.84e-05     -1.998      0.046      -0.000   -2.59e-06\n",
       "x133          -0.0001   6.84e-05     -2.006      0.045      -0.000   -3.09e-06\n",
       "x134          -0.0001   6.84e-05     -2.008      0.045      -0.000   -3.23e-06\n",
       "x135          -0.0001   6.84e-05     -2.018      0.044      -0.000   -3.96e-06\n",
       "x136          -0.0001   6.84e-05     -2.005      0.045      -0.000   -3.04e-06\n",
       "x137          -0.0001   6.84e-05     -2.006      0.045      -0.000   -3.12e-06\n",
       "x138           2.5772      2.509      1.027      0.304      -2.342       7.496\n",
       "x139        2.589e-09   1.32e-08      0.196      0.845   -2.33e-08    2.85e-08\n",
       "x140       -1.236e-08   1.17e-08     -1.059      0.290   -3.52e-08    1.05e-08\n",
       "x141           0.0046      0.037      0.124      0.901      -0.067       0.077\n",
       "x142       -4.494e-08   1.31e-08     -3.440      0.001   -7.06e-08   -1.93e-08\n",
       "x143           0.5603      0.539      1.039      0.299      -0.497       1.618\n",
       "x144           0.6934      0.544      1.274      0.203      -0.374       1.761\n",
       "x145           0.5697      0.540      1.055      0.292      -0.489       1.629\n",
       "x146           0.6108      0.540      1.131      0.258      -0.448       1.669\n",
       "x147           0.6295      0.541      1.163      0.245      -0.431       1.690\n",
       "x148           0.0007      0.000      5.831      0.000       0.000       0.001\n",
       "x149          -0.0013      0.000     -3.084      0.002      -0.002      -0.000\n",
       "x150           0.0001   8.77e-05      1.155      0.248   -7.06e-05       0.000\n",
       "x151        -9.45e-05   4.39e-05     -2.154      0.031      -0.000   -8.48e-06\n",
       "x152          -0.0004      0.000     -2.913      0.004      -0.001      -0.000\n",
       "x153        3.813e-08    1.5e-08      2.546      0.011    8.77e-09    6.75e-08\n",
       "x154          -0.3467      0.518     -0.669      0.503      -1.362       0.669\n",
       "x155          -1.0078      0.561     -1.795      0.073      -2.108       0.093\n",
       "x156          -1.1416      0.618     -1.846      0.065      -2.354       0.071\n",
       "x157          -0.3176      0.523     -0.607      0.544      -1.344       0.709\n",
       "x158          -0.9774      0.523     -1.869      0.062      -2.002       0.048\n",
       "x159          -0.6344      0.519     -1.222      0.222      -1.652       0.384\n",
       "x160          -0.2460      0.523     -0.470      0.638      -1.271       0.779\n",
       "x161           0.0003      0.000      1.475      0.140   -9.27e-05       0.001\n",
       "x162           0.0020      0.001      3.322      0.001       0.001       0.003\n",
       "x163           0.0033      0.001      2.845      0.004       0.001       0.006\n",
       "x164           0.0004      0.000      1.421      0.155      -0.000       0.001\n",
       "x165           0.0012      0.000      3.040      0.002       0.000       0.002\n",
       "x166           0.0002      0.000      0.828      0.408      -0.000       0.001\n",
       "x167          -0.0008      0.000     -2.790      0.005      -0.001      -0.000\n",
       "x168           0.1548      0.125      1.241      0.215      -0.090       0.399\n",
       "x169          -0.1908      0.150     -1.273      0.203      -0.485       0.103\n",
       "x170        1.926e-05   5.52e-06      3.492      0.000    8.45e-06    3.01e-05\n",
       "x171           0.0558      0.021      2.599      0.009       0.014       0.098\n",
       "x172       -3.891e-07   2.44e-07     -1.597      0.110   -8.67e-07    8.85e-08\n",
       "x173        3.601e-07   8.16e-07      0.441      0.659   -1.24e-06    1.96e-06\n",
       "x174        9.141e-07   5.44e-07      1.682      0.093   -1.52e-07    1.98e-06\n",
       "x175       -2.953e-07   7.24e-07     -0.408      0.684   -1.72e-06    1.12e-06\n",
       "x176       -1.448e-08   8.97e-08     -0.161      0.872    -1.9e-07    1.61e-07\n",
       "x177        5.481e-07   6.32e-07      0.867      0.386   -6.92e-07    1.79e-06\n",
       "x178        3.567e-07   1.06e-06      0.337      0.736   -1.72e-06    2.43e-06\n",
       "x179        1.128e-06   1.37e-06      0.821      0.412   -1.57e-06    3.82e-06\n",
       "x180       -6.818e-07   1.81e-06     -0.376      0.707   -4.23e-06    2.87e-06\n",
       "x181        -1.06e-08   1.46e-07     -0.072      0.942   -2.97e-07    2.76e-07\n",
       "x182         -1.6e-06   1.31e-06     -1.220      0.222   -4.17e-06    9.71e-07\n",
       "x183        2.451e-06   4.73e-06      0.518      0.604   -6.82e-06    1.17e-05\n",
       "x184        2.264e-06   6.62e-06      0.342      0.732   -1.07e-05    1.52e-05\n",
       "x185        4.714e-07   8.48e-06      0.056      0.956   -1.61e-05    1.71e-05\n",
       "x186       -5.587e-08   4.85e-07     -0.115      0.908   -1.01e-06    8.94e-07\n",
       "x187       -5.168e-07   5.06e-07     -1.022      0.307   -1.51e-06    4.74e-07\n",
       "x188       -9.773e-07   1.03e-06     -0.950      0.342   -2.99e-06    1.04e-06\n",
       "x189        1.772e-06   8.17e-07      2.169      0.030     1.7e-07    3.37e-06\n",
       "x190        5.231e-07   1.53e-06      0.343      0.732   -2.47e-06    3.52e-06\n",
       "x191        2.714e-07   1.55e-07      1.752      0.080   -3.23e-08    5.75e-07\n",
       "x192       -1.201e-06   9.77e-07     -1.230      0.219   -3.12e-06    7.13e-07\n",
       "x193         1.37e-06      1e-06      1.364      0.173   -5.99e-07    3.34e-06\n",
       "x194       -1.446e-06   1.16e-06     -1.252      0.211   -3.71e-06    8.19e-07\n",
       "x195        1.262e-06   1.62e-06      0.779      0.436   -1.92e-06    4.44e-06\n",
       "x196        4.902e-07    1.5e-07      3.269      0.001    1.96e-07    7.84e-07\n",
       "x197       -2.985e-08    3.7e-07     -0.081      0.936   -7.56e-07    6.96e-07\n",
       "x198        1.138e-06    7.4e-07      1.539      0.124   -3.12e-07    2.59e-06\n",
       "x199        3.721e-08   7.66e-07      0.049      0.961   -1.47e-06    1.54e-06\n",
       "x200       -1.228e-08   7.17e-07     -0.017      0.986   -1.42e-06    1.39e-06\n",
       "x201       -5.413e-08   1.43e-07     -0.379      0.705   -3.34e-07    2.26e-07\n",
       "x202       -2.697e-07   5.98e-07     -0.451      0.652   -1.44e-06    9.03e-07\n",
       "x203         9.23e-07   7.11e-07      1.298      0.194   -4.71e-07    2.32e-06\n",
       "x204        1.489e-07   5.21e-07      0.286      0.775   -8.72e-07    1.17e-06\n",
       "x205        1.111e-08   6.45e-07      0.017      0.986   -1.25e-06    1.28e-06\n",
       "x206        3.158e-07   1.87e-07      1.688      0.091   -5.09e-08    6.82e-07\n",
       "x207         3.42e-08    3.6e-07      0.095      0.924   -6.71e-07     7.4e-07\n",
       "x208        8.613e-07   5.79e-07      1.487      0.137   -2.74e-07       2e-06\n",
       "x209       -6.485e-07   9.87e-07     -0.657      0.511   -2.58e-06    1.29e-06\n",
       "x210        1.149e-06    9.8e-07      1.171      0.242   -7.74e-07    3.07e-06\n",
       "x211         1.76e-06   1.91e-06      0.920      0.358   -1.99e-06    5.51e-06\n",
       "x212        -3.99e-07   1.66e-07     -2.402      0.016   -7.25e-07   -7.33e-08\n",
       "x213       -9.814e-07   6.99e-07     -1.405      0.160   -2.35e-06    3.88e-07\n",
       "x214       -6.133e-06   4.05e-06     -1.513      0.130   -1.41e-05    1.81e-06\n",
       "x215        5.568e-06   2.55e-06      2.183      0.029    5.68e-07    1.06e-05\n",
       "x216       -5.652e-06   6.53e-06     -0.866      0.387   -1.84e-05    7.14e-06\n",
       "x217        -2.54e-08   2.11e-08     -1.206      0.228   -6.67e-08    1.59e-08\n",
       "x218                0          0        nan        nan           0           0\n",
       "x219       -2.148e-06   1.07e-06     -2.006      0.045   -4.25e-06   -4.91e-08\n",
       "x220        3.407e-06   1.89e-06      1.806      0.071   -2.91e-07     7.1e-06\n",
       "x221        4.124e-07   5.97e-07      0.691      0.490   -7.58e-07    1.58e-06\n",
       "x222        8.338e-07   8.32e-07      1.002      0.316   -7.97e-07    2.47e-06\n",
       "x223       -1.792e-06   1.12e-06     -1.603      0.109   -3.98e-06    3.99e-07\n",
       "x224       -6.512e-07   2.93e-07     -2.220      0.026   -1.23e-06   -7.62e-08\n",
       "x225        4.453e-06   1.25e-06      3.576      0.000    2.01e-06    6.89e-06\n",
       "x226        8.512e-07   1.62e-06      0.526      0.599   -2.32e-06    4.03e-06\n",
       "x227       -1.839e-05   7.19e-06     -2.557      0.011   -3.25e-05   -4.29e-06\n",
       "x228        2.387e-05   9.29e-06      2.569      0.010    5.65e-06    4.21e-05\n",
       "x229       -2.726e-07   1.88e-07     -1.447      0.148   -6.42e-07    9.67e-08\n",
       "x230       -2.002e-10   2.51e-10     -0.798      0.425   -6.92e-10    2.92e-10\n",
       "x231       -5.607e-08   2.59e-08     -2.169      0.030   -1.07e-07   -5.38e-09\n",
       "==============================================================================\n",
       "Omnibus:                      394.001   Durbin-Watson:                   1.980\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              696.174\n",
       "Skew:                           0.620   Prob(JB):                    6.73e-152\n",
       "Kurtosis:                       4.475   Cond. No.                     1.08e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.08e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_private = sm.OLS(y_train_private_continuous, X_train_private_continuous).fit()\n",
    "model_private.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>GBA6RTBK</td>     <th>  R-squared:         </th> <td>   0.470</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.435</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   13.46</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 01 Dec 2021</td> <th>  Prob (F-statistic):</th> <td>3.15e-232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:22:55</td>     <th>  Log-Likelihood:    </th> <td> -10624.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  2574</td>      <th>  AIC:               </th> <td>2.157e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  2414</td>      <th>  BIC:               </th> <td>2.250e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   159</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td> 1.606e-05</td> <td> 5.42e-06</td> <td>    2.965</td> <td> 0.003</td> <td> 5.44e-06</td> <td> 2.67e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>-1.108e-06</td> <td> 1.15e-06</td> <td>   -0.962</td> <td> 0.336</td> <td>-3.37e-06</td> <td> 1.15e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>-8.431e-07</td> <td> 8.19e-07</td> <td>   -1.029</td> <td> 0.304</td> <td>-2.45e-06</td> <td> 7.64e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>-8.409e-07</td> <td>  8.2e-07</td> <td>   -1.026</td> <td> 0.305</td> <td>-2.45e-06</td> <td> 7.67e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>-2.736e-07</td> <td> 3.48e-07</td> <td>   -0.787</td> <td> 0.431</td> <td>-9.55e-07</td> <td> 4.08e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td> 8.763e-07</td> <td> 1.16e-06</td> <td>    0.757</td> <td> 0.449</td> <td>-1.39e-06</td> <td> 3.15e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>-2.407e-07</td> <td> 2.97e-07</td> <td>   -0.811</td> <td> 0.418</td> <td>-8.23e-07</td> <td> 3.41e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>-1.018e-07</td> <td>  1.8e-07</td> <td>   -0.566</td> <td> 0.571</td> <td>-4.54e-07</td> <td> 2.51e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>-4.348e-08</td> <td>  1.8e-07</td> <td>   -0.242</td> <td> 0.809</td> <td>-3.96e-07</td> <td> 3.09e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>-4.389e-08</td> <td> 1.51e-07</td> <td>   -0.290</td> <td> 0.772</td> <td> -3.4e-07</td> <td> 2.53e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td> 4.437e-07</td> <td> 2.75e-07</td> <td>    1.611</td> <td> 0.107</td> <td>-9.65e-08</td> <td> 9.84e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td> 4.327e-07</td> <td> 2.76e-07</td> <td>    1.568</td> <td> 0.117</td> <td>-1.08e-07</td> <td> 9.74e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>-5.119e-07</td> <td> 5.24e-07</td> <td>   -0.977</td> <td> 0.329</td> <td>-1.54e-06</td> <td> 5.16e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td> 3.115e-07</td> <td> 3.48e-07</td> <td>    0.896</td> <td> 0.370</td> <td> -3.7e-07</td> <td> 9.93e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td> 2.364e-07</td> <td> 3.02e-07</td> <td>    0.784</td> <td> 0.433</td> <td>-3.55e-07</td> <td> 8.28e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>-8.914e-07</td> <td> 4.29e-07</td> <td>   -2.077</td> <td> 0.038</td> <td>-1.73e-06</td> <td>-4.99e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>-8.564e-07</td> <td> 4.29e-07</td> <td>   -1.995</td> <td> 0.046</td> <td> -1.7e-06</td> <td>-1.47e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>-9.138e-07</td> <td> 4.29e-07</td> <td>   -2.129</td> <td> 0.033</td> <td>-1.76e-06</td> <td>-7.23e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>-8.889e-07</td> <td> 4.29e-07</td> <td>   -2.071</td> <td> 0.039</td> <td>-1.73e-06</td> <td>-4.71e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>-3.701e-06</td> <td> 1.56e-06</td> <td>   -2.378</td> <td> 0.017</td> <td>-6.75e-06</td> <td>-6.49e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>-4.244e-07</td> <td> 3.74e-07</td> <td>   -1.135</td> <td> 0.257</td> <td>-1.16e-06</td> <td> 3.09e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td> -3.97e-07</td> <td> 3.74e-07</td> <td>   -1.062</td> <td> 0.288</td> <td>-1.13e-06</td> <td> 3.36e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>-4.037e-07</td> <td> 3.74e-07</td> <td>   -1.080</td> <td> 0.280</td> <td>-1.14e-06</td> <td> 3.29e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>-4.087e-07</td> <td> 3.74e-07</td> <td>   -1.093</td> <td> 0.275</td> <td>-1.14e-06</td> <td> 3.25e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>-3.971e-07</td> <td> 3.73e-07</td> <td>   -1.063</td> <td> 0.288</td> <td>-1.13e-06</td> <td> 3.35e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td> 4.133e-07</td> <td> 3.73e-07</td> <td>    1.107</td> <td> 0.268</td> <td>-3.19e-07</td> <td> 1.15e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>-2.599e-08</td> <td> 8.59e-09</td> <td>   -3.026</td> <td> 0.003</td> <td>-4.28e-08</td> <td>-9.15e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>-7.117e-08</td> <td> 7.18e-08</td> <td>   -0.992</td> <td> 0.321</td> <td>-2.12e-07</td> <td> 6.95e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>  1.48e-08</td> <td> 1.58e-08</td> <td>    0.934</td> <td> 0.350</td> <td>-1.63e-08</td> <td> 4.59e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td> 3.681e-06</td> <td> 1.19e-06</td> <td>    3.095</td> <td> 0.002</td> <td> 1.35e-06</td> <td> 6.01e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>-3.678e-06</td> <td> 1.19e-06</td> <td>   -3.094</td> <td> 0.002</td> <td>-6.01e-06</td> <td>-1.35e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td> 6.156e-06</td> <td> 2.28e-06</td> <td>    2.695</td> <td> 0.007</td> <td> 1.68e-06</td> <td> 1.06e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td> 9.836e-06</td> <td> 3.38e-06</td> <td>    2.914</td> <td> 0.004</td> <td> 3.22e-06</td> <td> 1.65e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td> 9.837e-06</td> <td> 3.38e-06</td> <td>    2.915</td> <td> 0.004</td> <td> 3.22e-06</td> <td> 1.65e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>-5.016e-06</td> <td> 1.62e-06</td> <td>   -3.092</td> <td> 0.002</td> <td> -8.2e-06</td> <td>-1.83e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>  -4.9e-07</td> <td> 1.57e-07</td> <td>   -3.121</td> <td> 0.002</td> <td>-7.98e-07</td> <td>-1.82e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>-4.468e-07</td> <td>  1.6e-07</td> <td>   -2.795</td> <td> 0.005</td> <td> -7.6e-07</td> <td>-1.33e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>-4.349e-07</td> <td> 1.64e-07</td> <td>   -2.647</td> <td> 0.008</td> <td>-7.57e-07</td> <td>-1.13e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>-2.791e-07</td> <td> 1.12e-07</td> <td>   -2.502</td> <td> 0.012</td> <td>-4.98e-07</td> <td>-6.04e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>-9.391e-08</td> <td> 8.93e-08</td> <td>   -1.052</td> <td> 0.293</td> <td>-2.69e-07</td> <td> 8.12e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>-1.852e-07</td> <td> 6.79e-08</td> <td>   -2.729</td> <td> 0.006</td> <td>-3.18e-07</td> <td>-5.21e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td> -3.65e-07</td> <td> 1.59e-07</td> <td>   -2.291</td> <td> 0.022</td> <td>-6.77e-07</td> <td>-5.26e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td>-4.355e-07</td> <td> 1.59e-07</td> <td>   -2.742</td> <td> 0.006</td> <td>-7.47e-07</td> <td>-1.24e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>-4.907e-07</td> <td> 1.57e-07</td> <td>   -3.120</td> <td> 0.002</td> <td>-7.99e-07</td> <td>-1.82e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>-9.634e-07</td> <td> 2.11e-07</td> <td>   -4.577</td> <td> 0.000</td> <td>-1.38e-06</td> <td>-5.51e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>-4.807e-07</td> <td> 1.58e-07</td> <td>   -3.046</td> <td> 0.002</td> <td> -7.9e-07</td> <td>-1.71e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td> 3.013e-07</td> <td> 9.62e-08</td> <td>    3.132</td> <td> 0.002</td> <td> 1.13e-07</td> <td>  4.9e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td> 1.104e-07</td> <td> 3.78e-08</td> <td>    2.919</td> <td> 0.004</td> <td> 3.62e-08</td> <td> 1.85e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td> 7.469e-08</td> <td> 3.51e-08</td> <td>    2.125</td> <td> 0.034</td> <td> 5.77e-09</td> <td> 1.44e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td>-8.427e-08</td> <td> 1.43e-07</td> <td>   -0.590</td> <td> 0.555</td> <td>-3.64e-07</td> <td> 1.96e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td> 1.099e-07</td> <td>  1.4e-07</td> <td>    0.785</td> <td> 0.433</td> <td>-1.65e-07</td> <td> 3.85e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td> 5.557e-09</td> <td> 5.19e-08</td> <td>    0.107</td> <td> 0.915</td> <td>-9.62e-08</td> <td> 1.07e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td>-4.295e-07</td> <td> 2.14e-07</td> <td>   -2.004</td> <td> 0.045</td> <td> -8.5e-07</td> <td>-9.33e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th>   <td>-3.066e-08</td> <td> 4.19e-08</td> <td>   -0.732</td> <td> 0.464</td> <td>-1.13e-07</td> <td> 5.15e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th>   <td> 3.403e-08</td> <td> 4.07e-08</td> <td>    0.836</td> <td> 0.403</td> <td>-4.58e-08</td> <td> 1.14e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th>   <td> 1.562e-09</td> <td> 3.47e-08</td> <td>    0.045</td> <td> 0.964</td> <td>-6.65e-08</td> <td> 6.97e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th>   <td>-2.083e-07</td> <td> 5.88e-08</td> <td>   -3.544</td> <td> 0.000</td> <td>-3.24e-07</td> <td> -9.3e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th>   <td> 9.299e-08</td> <td> 4.25e-08</td> <td>    2.190</td> <td> 0.029</td> <td> 9.74e-09</td> <td> 1.76e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th>   <td>-9.923e-08</td> <td>  3.8e-08</td> <td>   -2.608</td> <td> 0.009</td> <td>-1.74e-07</td> <td>-2.46e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th>   <td>-8.187e-08</td> <td> 4.24e-08</td> <td>   -1.930</td> <td> 0.054</td> <td>-1.65e-07</td> <td> 1.31e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th>   <td> 2.667e-07</td> <td> 1.06e-07</td> <td>    2.512</td> <td> 0.012</td> <td> 5.85e-08</td> <td> 4.75e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th>   <td>-8.967e-08</td> <td> 3.24e-08</td> <td>   -2.765</td> <td> 0.006</td> <td>-1.53e-07</td> <td>-2.61e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th>   <td>-4.028e-09</td> <td>  2.7e-08</td> <td>   -0.149</td> <td> 0.881</td> <td>-5.69e-08</td> <td> 4.89e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x64</th>   <td> 8.895e-08</td> <td> 2.91e-08</td> <td>    3.058</td> <td> 0.002</td> <td> 3.19e-08</td> <td> 1.46e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x65</th>   <td>-4.421e-07</td> <td> 1.26e-07</td> <td>   -3.518</td> <td> 0.000</td> <td>-6.89e-07</td> <td>-1.96e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x66</th>   <td>-3.302e-07</td> <td>  4.1e-07</td> <td>   -0.805</td> <td> 0.421</td> <td>-1.13e-06</td> <td> 4.75e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x67</th>   <td>-6.563e-07</td> <td> 1.27e-07</td> <td>   -5.165</td> <td> 0.000</td> <td>-9.06e-07</td> <td>-4.07e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x68</th>   <td> 6.195e-08</td> <td>  4.2e-07</td> <td>    0.148</td> <td> 0.883</td> <td>-7.62e-07</td> <td> 8.85e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x69</th>   <td>-3.677e-07</td> <td> 1.33e-07</td> <td>   -2.764</td> <td> 0.006</td> <td>-6.29e-07</td> <td>-1.07e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x70</th>   <td>-3.539e-07</td> <td> 4.23e-07</td> <td>   -0.836</td> <td> 0.403</td> <td>-1.18e-06</td> <td> 4.76e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x71</th>   <td>-3.925e-07</td> <td> 1.32e-07</td> <td>   -2.968</td> <td> 0.003</td> <td>-6.52e-07</td> <td>-1.33e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x72</th>   <td>-4.628e-07</td> <td> 4.14e-07</td> <td>   -1.117</td> <td> 0.264</td> <td>-1.28e-06</td> <td>  3.5e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x73</th>   <td> -3.59e-07</td> <td> 1.51e-07</td> <td>   -2.382</td> <td> 0.017</td> <td>-6.54e-07</td> <td>-6.34e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x74</th>   <td> -4.91e-07</td> <td> 4.65e-07</td> <td>   -1.056</td> <td> 0.291</td> <td> -1.4e-06</td> <td> 4.21e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x75</th>   <td>-3.976e-07</td> <td> 1.26e-07</td> <td>   -3.155</td> <td> 0.002</td> <td>-6.45e-07</td> <td> -1.5e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x76</th>   <td>-3.293e-07</td> <td> 4.18e-07</td> <td>   -0.787</td> <td> 0.431</td> <td>-1.15e-06</td> <td> 4.91e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x77</th>   <td>-2.046e-07</td> <td> 5.27e-08</td> <td>   -3.882</td> <td> 0.000</td> <td>-3.08e-07</td> <td>-1.01e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x78</th>   <td>-3.765e-07</td> <td> 1.25e-07</td> <td>   -3.009</td> <td> 0.003</td> <td>-6.22e-07</td> <td>-1.31e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x79</th>   <td>-5.693e-07</td> <td> 4.13e-07</td> <td>   -1.379</td> <td> 0.168</td> <td>-1.38e-06</td> <td>  2.4e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x80</th>   <td>-4.779e-07</td> <td> 1.22e-07</td> <td>   -3.906</td> <td> 0.000</td> <td>-7.18e-07</td> <td>-2.38e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x81</th>   <td>-3.459e-07</td> <td> 4.11e-07</td> <td>   -0.842</td> <td> 0.400</td> <td>-1.15e-06</td> <td> 4.59e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x82</th>   <td> 1.093e-07</td> <td> 2.15e-07</td> <td>    0.508</td> <td> 0.612</td> <td>-3.13e-07</td> <td> 5.32e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x83</th>   <td>-9.932e-07</td> <td> 4.55e-07</td> <td>   -2.182</td> <td> 0.029</td> <td>-1.89e-06</td> <td>-1.01e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x84</th>   <td>-4.411e-07</td> <td> 1.22e-07</td> <td>   -3.609</td> <td> 0.000</td> <td>-6.81e-07</td> <td>-2.01e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x85</th>   <td>-6.546e-08</td> <td> 4.25e-07</td> <td>   -0.154</td> <td> 0.878</td> <td>-8.99e-07</td> <td> 7.68e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x86</th>   <td> 4.224e-07</td> <td> 1.34e-07</td> <td>    3.145</td> <td> 0.002</td> <td> 1.59e-07</td> <td> 6.86e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x87</th>   <td> 3.157e-07</td> <td> 4.24e-07</td> <td>    0.744</td> <td> 0.457</td> <td>-5.17e-07</td> <td> 1.15e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x88</th>   <td> 6.326e-09</td> <td> 3.15e-08</td> <td>    0.201</td> <td> 0.841</td> <td>-5.54e-08</td> <td>  6.8e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x89</th>   <td> 7.028e-08</td> <td> 3.74e-08</td> <td>    1.877</td> <td> 0.061</td> <td>-3.13e-09</td> <td> 1.44e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x90</th>   <td> 3.074e-07</td> <td> 6.74e-08</td> <td>    4.560</td> <td> 0.000</td> <td> 1.75e-07</td> <td>  4.4e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x91</th>   <td>-1.163e-07</td> <td> 1.27e-07</td> <td>   -0.916</td> <td> 0.360</td> <td>-3.66e-07</td> <td> 1.33e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x92</th>   <td> 2.084e-08</td> <td> 2.36e-08</td> <td>    0.883</td> <td> 0.377</td> <td>-2.54e-08</td> <td> 6.71e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x93</th>   <td> -2.39e-07</td> <td> 1.89e-07</td> <td>   -1.262</td> <td> 0.207</td> <td> -6.1e-07</td> <td> 1.32e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x94</th>   <td> -1.31e-08</td> <td> 2.62e-08</td> <td>   -0.500</td> <td> 0.617</td> <td>-6.44e-08</td> <td> 3.82e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x95</th>   <td>-6.722e-09</td> <td> 5.32e-09</td> <td>   -1.264</td> <td> 0.206</td> <td>-1.71e-08</td> <td>  3.7e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x96</th>   <td>-7.869e-09</td> <td> 2.09e-08</td> <td>   -0.376</td> <td> 0.707</td> <td>-4.89e-08</td> <td> 3.31e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x97</th>   <td> 5.768e-08</td> <td> 2.52e-08</td> <td>    2.286</td> <td> 0.022</td> <td> 8.21e-09</td> <td> 1.07e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x98</th>   <td>-2.257e-09</td> <td> 8.61e-08</td> <td>   -0.026</td> <td> 0.979</td> <td>-1.71e-07</td> <td> 1.67e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x99</th>   <td> 2.287e-09</td> <td> 9.31e-09</td> <td>    0.246</td> <td> 0.806</td> <td> -1.6e-08</td> <td> 2.06e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x100</th>  <td>-3.718e-08</td> <td> 2.61e-08</td> <td>   -1.424</td> <td> 0.154</td> <td>-8.84e-08</td> <td>  1.4e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x101</th>  <td>-8.966e-09</td> <td> 5.84e-08</td> <td>   -0.154</td> <td> 0.878</td> <td>-1.23e-07</td> <td> 1.06e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x102</th>  <td>-2.238e-07</td> <td> 1.51e-07</td> <td>   -1.487</td> <td> 0.137</td> <td>-5.19e-07</td> <td> 7.14e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x103</th>  <td>-4.351e-07</td> <td> 1.38e-07</td> <td>   -3.158</td> <td> 0.002</td> <td>-7.05e-07</td> <td>-1.65e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x104</th>  <td> 6.598e-08</td> <td> 8.01e-08</td> <td>    0.824</td> <td> 0.410</td> <td> -9.1e-08</td> <td> 2.23e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x105</th>  <td> 8.436e-07</td> <td> 3.26e-07</td> <td>    2.585</td> <td> 0.010</td> <td> 2.04e-07</td> <td> 1.48e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x106</th>  <td> -1.29e-07</td> <td> 8.75e-08</td> <td>   -1.475</td> <td> 0.140</td> <td>-3.01e-07</td> <td> 4.25e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x107</th>  <td> -9.39e-08</td> <td> 7.22e-08</td> <td>   -1.301</td> <td> 0.193</td> <td>-2.35e-07</td> <td> 4.77e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x108</th>  <td> 2.768e-08</td> <td> 4.86e-08</td> <td>    0.569</td> <td> 0.569</td> <td>-6.76e-08</td> <td> 1.23e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x109</th>  <td>-6.092e-08</td> <td> 4.01e-08</td> <td>   -1.517</td> <td> 0.129</td> <td> -1.4e-07</td> <td> 1.78e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x110</th>  <td> 2.889e-07</td> <td> 5.72e-08</td> <td>    5.054</td> <td> 0.000</td> <td> 1.77e-07</td> <td> 4.01e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x111</th>  <td>  2.28e-07</td> <td> 3.71e-08</td> <td>    6.145</td> <td> 0.000</td> <td> 1.55e-07</td> <td> 3.01e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x112</th>  <td>-2.003e-07</td> <td>  5.2e-08</td> <td>   -3.853</td> <td> 0.000</td> <td>-3.02e-07</td> <td>-9.84e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x113</th>  <td>-2.118e-08</td> <td> 2.36e-08</td> <td>   -0.898</td> <td> 0.369</td> <td>-6.74e-08</td> <td> 2.51e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x114</th>  <td> 2.424e-09</td> <td> 7.19e-09</td> <td>    0.337</td> <td> 0.736</td> <td>-1.17e-08</td> <td> 1.65e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x115</th>  <td>-3.444e-09</td> <td> 6.87e-09</td> <td>   -0.501</td> <td> 0.616</td> <td>-1.69e-08</td> <td>    1e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x116</th>  <td> 6.191e-08</td> <td>  1.2e-07</td> <td>    0.516</td> <td> 0.606</td> <td>-1.74e-07</td> <td> 2.97e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x117</th>  <td> 2.874e-09</td> <td> 1.61e-08</td> <td>    0.179</td> <td> 0.858</td> <td>-2.86e-08</td> <td> 3.44e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x118</th>  <td>  5.59e-06</td> <td>    2e-06</td> <td>    2.798</td> <td> 0.005</td> <td> 1.67e-06</td> <td> 9.51e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x119</th>  <td> 9.111e-06</td> <td> 6.29e-06</td> <td>    1.448</td> <td> 0.148</td> <td>-3.23e-06</td> <td> 2.15e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x120</th>  <td>-1.381e-05</td> <td>    1e-05</td> <td>   -1.379</td> <td> 0.168</td> <td>-3.34e-05</td> <td> 5.83e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x121</th>  <td> 4.841e-07</td> <td> 1.85e-06</td> <td>    0.262</td> <td> 0.793</td> <td>-3.14e-06</td> <td> 4.11e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x122</th>  <td>-4.586e-07</td> <td> 5.57e-07</td> <td>   -0.824</td> <td> 0.410</td> <td>-1.55e-06</td> <td> 6.33e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x123</th>  <td>-1.816e-06</td> <td> 2.14e-06</td> <td>   -0.849</td> <td> 0.396</td> <td>-6.01e-06</td> <td> 2.38e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x124</th>  <td> 8.098e-07</td> <td> 2.05e-06</td> <td>    0.395</td> <td> 0.693</td> <td>-3.21e-06</td> <td> 4.83e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x125</th>  <td>    0.0009</td> <td>    0.000</td> <td>    4.063</td> <td> 0.000</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x126</th>  <td>-6.116e-05</td> <td>    0.000</td> <td>   -0.337</td> <td> 0.736</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x127</th>  <td>   -0.0025</td> <td>    0.002</td> <td>   -1.388</td> <td> 0.165</td> <td>   -0.006</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x128</th>  <td>   -0.0004</td> <td>    0.000</td> <td>   -1.891</td> <td> 0.059</td> <td>   -0.001</td> <td> 1.49e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x129</th>  <td>   -0.0003</td> <td>    0.000</td> <td>   -0.889</td> <td> 0.374</td> <td>   -0.001</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x130</th>  <td>-1.476e-05</td> <td>    0.001</td> <td>   -0.021</td> <td> 0.983</td> <td>   -0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x131</th>  <td>   -0.0001</td> <td>    0.000</td> <td>   -1.042</td> <td> 0.297</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x132</th>  <td>-9.698e-09</td> <td> 1.74e-08</td> <td>   -0.556</td> <td> 0.578</td> <td>-4.39e-08</td> <td> 2.45e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x133</th>  <td>-1.356e-06</td> <td> 1.68e-06</td> <td>   -0.807</td> <td> 0.419</td> <td>-4.65e-06</td> <td> 1.94e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x134</th>  <td> 8.303e-07</td> <td> 8.39e-07</td> <td>    0.990</td> <td> 0.322</td> <td>-8.14e-07</td> <td> 2.47e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x135</th>  <td>-5.044e-06</td> <td> 1.28e-06</td> <td>   -3.927</td> <td> 0.000</td> <td>-7.56e-06</td> <td>-2.53e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x136</th>  <td>  1.03e-06</td> <td> 1.53e-06</td> <td>    0.673</td> <td> 0.501</td> <td>-1.97e-06</td> <td> 4.03e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x137</th>  <td> 2.139e-06</td> <td> 1.47e-06</td> <td>    1.459</td> <td> 0.145</td> <td>-7.37e-07</td> <td> 5.02e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x138</th>  <td> 2.706e-06</td> <td> 1.28e-06</td> <td>    2.107</td> <td> 0.035</td> <td> 1.88e-07</td> <td> 5.22e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x139</th>  <td>-2.141e-07</td> <td> 1.45e-06</td> <td>   -0.148</td> <td> 0.882</td> <td>-3.05e-06</td> <td> 2.63e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x140</th>  <td>    0.0006</td> <td>    0.000</td> <td>    2.328</td> <td> 0.020</td> <td> 9.49e-05</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x141</th>  <td>    0.0006</td> <td>    0.000</td> <td>    2.678</td> <td> 0.007</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x142</th>  <td>   -0.0015</td> <td>    0.000</td> <td>   -3.389</td> <td> 0.001</td> <td>   -0.002</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x143</th>  <td>    0.0008</td> <td>    0.000</td> <td>    1.694</td> <td> 0.090</td> <td>   -0.000</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x144</th>  <td>    0.0013</td> <td>    0.000</td> <td>    2.882</td> <td> 0.004</td> <td>    0.000</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x145</th>  <td>    0.0009</td> <td>    0.000</td> <td>    2.491</td> <td> 0.013</td> <td>    0.000</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x146</th>  <td>-8.885e-06</td> <td>    0.000</td> <td>   -0.036</td> <td> 0.972</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x147</th>  <td>-4.143e-06</td> <td>  2.2e-06</td> <td>   -1.885</td> <td> 0.060</td> <td>-8.45e-06</td> <td> 1.66e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x148</th>  <td>-5.068e-06</td> <td> 2.97e-06</td> <td>   -1.708</td> <td> 0.088</td> <td>-1.09e-05</td> <td> 7.52e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x149</th>  <td> 6.594e-05</td> <td> 3.74e-05</td> <td>    1.765</td> <td> 0.078</td> <td>-7.31e-06</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x150</th>  <td>-1.591e-05</td> <td> 8.74e-06</td> <td>   -1.820</td> <td> 0.069</td> <td> -3.3e-05</td> <td> 1.23e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x151</th>  <td>-5.655e-09</td> <td> 1.05e-07</td> <td>   -0.054</td> <td> 0.957</td> <td>-2.12e-07</td> <td> 2.01e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x152</th>  <td>  3.09e-06</td> <td> 1.47e-06</td> <td>    2.102</td> <td> 0.036</td> <td> 2.07e-07</td> <td> 5.97e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x153</th>  <td> 2.081e-06</td> <td> 9.99e-07</td> <td>    2.083</td> <td> 0.037</td> <td> 1.22e-07</td> <td> 4.04e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x154</th>  <td>-2.374e-06</td> <td> 1.25e-06</td> <td>   -1.898</td> <td> 0.058</td> <td>-4.83e-06</td> <td> 7.86e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x155</th>  <td>  5.72e-06</td> <td> 6.68e-06</td> <td>    0.857</td> <td> 0.392</td> <td>-7.37e-06</td> <td> 1.88e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x156</th>  <td> 1.543e-07</td> <td> 2.01e-07</td> <td>    0.768</td> <td> 0.442</td> <td>-2.39e-07</td> <td> 5.48e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x157</th>  <td> 2.675e-06</td> <td> 1.43e-06</td> <td>    1.865</td> <td> 0.062</td> <td>-1.38e-07</td> <td> 5.49e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x158</th>  <td> 2.419e-06</td> <td> 9.95e-07</td> <td>    2.431</td> <td> 0.015</td> <td> 4.67e-07</td> <td> 4.37e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x159</th>  <td>-2.357e-06</td> <td> 1.24e-06</td> <td>   -1.898</td> <td> 0.058</td> <td>-4.79e-06</td> <td> 7.77e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x160</th>  <td> 3.713e-06</td> <td> 6.74e-06</td> <td>    0.551</td> <td> 0.582</td> <td>-9.51e-06</td> <td> 1.69e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x161</th>  <td>-1.844e-07</td> <td> 3.65e-07</td> <td>   -0.505</td> <td> 0.614</td> <td>-9.01e-07</td> <td> 5.32e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x162</th>  <td>  1.93e-06</td> <td> 1.67e-06</td> <td>    1.156</td> <td> 0.248</td> <td>-1.34e-06</td> <td>  5.2e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x163</th>  <td> 2.339e-06</td> <td> 1.01e-06</td> <td>    2.309</td> <td> 0.021</td> <td> 3.52e-07</td> <td> 4.33e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x164</th>  <td> -1.83e-06</td> <td> 1.56e-06</td> <td>   -1.172</td> <td> 0.241</td> <td>-4.89e-06</td> <td> 1.23e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x165</th>  <td> 6.616e-06</td> <td> 7.32e-06</td> <td>    0.903</td> <td> 0.366</td> <td>-7.75e-06</td> <td>  2.1e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x166</th>  <td>-2.368e-07</td> <td> 2.28e-07</td> <td>   -1.037</td> <td> 0.300</td> <td>-6.85e-07</td> <td> 2.11e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x167</th>  <td> 3.305e-06</td> <td> 1.49e-06</td> <td>    2.213</td> <td> 0.027</td> <td> 3.76e-07</td> <td> 6.23e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x168</th>  <td> 2.063e-06</td> <td> 9.99e-07</td> <td>    2.065</td> <td> 0.039</td> <td> 1.04e-07</td> <td> 4.02e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x169</th>  <td>-1.741e-06</td> <td> 1.24e-06</td> <td>   -1.403</td> <td> 0.161</td> <td>-4.17e-06</td> <td> 6.92e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x170</th>  <td> 4.959e-06</td> <td> 6.71e-06</td> <td>    0.740</td> <td> 0.460</td> <td>-8.19e-06</td> <td> 1.81e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x171</th>  <td>-6.481e-07</td> <td> 5.59e-07</td> <td>   -1.158</td> <td> 0.247</td> <td>-1.75e-06</td> <td> 4.49e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x172</th>  <td> 4.501e-06</td> <td> 1.78e-06</td> <td>    2.530</td> <td> 0.011</td> <td> 1.01e-06</td> <td> 7.99e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x173</th>  <td> 2.232e-06</td> <td> 1.02e-06</td> <td>    2.189</td> <td> 0.029</td> <td> 2.33e-07</td> <td> 4.23e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x174</th>  <td>-1.108e-06</td> <td> 1.45e-06</td> <td>   -0.764</td> <td> 0.445</td> <td>-3.95e-06</td> <td> 1.74e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x175</th>  <td> 5.722e-06</td> <td> 6.82e-06</td> <td>    0.839</td> <td> 0.402</td> <td>-7.66e-06</td> <td> 1.91e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x176</th>  <td> 1.605e-07</td> <td> 2.51e-07</td> <td>    0.640</td> <td> 0.522</td> <td>-3.31e-07</td> <td> 6.52e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x177</th>  <td> 2.913e-06</td> <td> 1.55e-06</td> <td>    1.878</td> <td> 0.060</td> <td>-1.28e-07</td> <td> 5.95e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x178</th>  <td> 2.103e-06</td> <td>    1e-06</td> <td>    2.098</td> <td> 0.036</td> <td> 1.38e-07</td> <td> 4.07e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x179</th>  <td>-2.299e-06</td> <td> 1.26e-06</td> <td>   -1.822</td> <td> 0.069</td> <td>-4.77e-06</td> <td> 1.75e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x180</th>  <td> 2.988e-06</td> <td> 6.78e-06</td> <td>    0.441</td> <td> 0.660</td> <td>-1.03e-05</td> <td> 1.63e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x181</th>  <td>-2.395e-06</td> <td> 1.12e-06</td> <td>   -2.133</td> <td> 0.033</td> <td> -4.6e-06</td> <td>-1.93e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x182</th>  <td>-3.653e-07</td> <td> 1.14e-06</td> <td>   -0.320</td> <td> 0.749</td> <td>-2.61e-06</td> <td> 1.87e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x183</th>  <td> 1.083e-06</td> <td> 1.75e-06</td> <td>    0.620</td> <td> 0.536</td> <td>-2.34e-06</td> <td> 4.51e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x184</th>  <td> 4.911e-07</td> <td> 1.33e-06</td> <td>    0.368</td> <td> 0.713</td> <td>-2.12e-06</td> <td> 3.11e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x185</th>  <td>-3.858e-06</td> <td> 2.02e-06</td> <td>   -1.913</td> <td> 0.056</td> <td>-7.81e-06</td> <td> 9.62e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x186</th>  <td> 5.045e-06</td> <td> 5.86e-06</td> <td>    0.861</td> <td> 0.389</td> <td>-6.45e-06</td> <td> 1.65e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x187</th>  <td> 2.265e-06</td> <td> 9.94e-07</td> <td>    2.279</td> <td> 0.023</td> <td> 3.16e-07</td> <td> 4.21e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x188</th>  <td>-9.934e-08</td> <td> 2.35e-07</td> <td>   -0.422</td> <td> 0.673</td> <td>-5.61e-07</td> <td> 3.62e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x189</th>  <td> 2.763e-06</td> <td> 1.38e-06</td> <td>    2.006</td> <td> 0.045</td> <td> 6.18e-08</td> <td> 5.46e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x190</th>  <td> 2.279e-06</td> <td> 9.96e-07</td> <td>    2.289</td> <td> 0.022</td> <td> 3.27e-07</td> <td> 4.23e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x191</th>  <td>-1.863e-06</td> <td> 1.22e-06</td> <td>   -1.528</td> <td> 0.127</td> <td>-4.25e-06</td> <td> 5.28e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x192</th>  <td> 5.228e-06</td> <td> 6.63e-06</td> <td>    0.789</td> <td> 0.430</td> <td>-7.77e-06</td> <td> 1.82e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x193</th>  <td>-2.927e-08</td> <td>  9.2e-08</td> <td>   -0.318</td> <td> 0.750</td> <td> -2.1e-07</td> <td> 1.51e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x194</th>  <td> 2.986e-06</td> <td> 1.53e-06</td> <td>    1.951</td> <td> 0.051</td> <td> -1.5e-08</td> <td> 5.99e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x195</th>  <td> 2.203e-06</td> <td> 9.97e-07</td> <td>    2.209</td> <td> 0.027</td> <td> 2.48e-07</td> <td> 4.16e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x196</th>  <td>-1.924e-06</td> <td> 1.26e-06</td> <td>   -1.524</td> <td> 0.128</td> <td> -4.4e-06</td> <td> 5.52e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x197</th>  <td> 6.044e-06</td> <td>  6.8e-06</td> <td>    0.888</td> <td> 0.374</td> <td> -7.3e-06</td> <td> 1.94e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x198</th>  <td>-4.593e-07</td> <td>  4.7e-07</td> <td>   -0.978</td> <td> 0.328</td> <td>-1.38e-06</td> <td> 4.62e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x199</th>  <td>-2.534e-05</td> <td> 1.52e-05</td> <td>   -1.668</td> <td> 0.095</td> <td>-5.51e-05</td> <td> 4.45e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x200</th>  <td> 4.171e-06</td> <td> 1.25e-06</td> <td>    3.331</td> <td> 0.001</td> <td> 1.72e-06</td> <td> 6.63e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x201</th>  <td> 2.411e-05</td> <td> 1.26e-05</td> <td>    1.906</td> <td> 0.057</td> <td>-6.91e-07</td> <td> 4.89e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x202</th>  <td>-5.585e-05</td> <td> 7.25e-05</td> <td>   -0.771</td> <td> 0.441</td> <td>   -0.000</td> <td> 8.63e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x203</th>  <td>-6.458e-07</td> <td> 8.84e-07</td> <td>   -0.730</td> <td> 0.465</td> <td>-2.38e-06</td> <td> 1.09e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x204</th>  <td> 7.682e-07</td> <td> 3.12e-06</td> <td>    0.246</td> <td> 0.806</td> <td>-5.35e-06</td> <td> 6.89e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x205</th>  <td> 2.141e-06</td> <td> 9.95e-07</td> <td>    2.152</td> <td> 0.032</td> <td>  1.9e-07</td> <td> 4.09e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x206</th>  <td>-2.998e-06</td> <td> 1.77e-06</td> <td>   -1.690</td> <td> 0.091</td> <td>-6.48e-06</td> <td> 4.82e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x207</th>  <td> 5.356e-06</td> <td> 6.98e-06</td> <td>    0.767</td> <td> 0.443</td> <td>-8.33e-06</td> <td>  1.9e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x208</th>  <td>-2.657e-06</td> <td> 1.39e-06</td> <td>   -1.916</td> <td> 0.055</td> <td>-5.38e-06</td> <td> 6.17e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x209</th>  <td>  -2.2e-06</td> <td> 9.94e-07</td> <td>   -2.214</td> <td> 0.027</td> <td>-4.15e-06</td> <td>-2.51e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x210</th>  <td>         0</td> <td>        0</td> <td>      nan</td> <td>   nan</td> <td>        0</td> <td>        0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x211</th>  <td>-5.215e-06</td> <td> 6.66e-06</td> <td>   -0.783</td> <td> 0.434</td> <td>-1.83e-05</td> <td> 7.84e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x212</th>  <td> 1.118e-16</td> <td>    2e-16</td> <td>    0.559</td> <td> 0.576</td> <td>-2.81e-16</td> <td> 5.04e-16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x213</th>  <td>-7.049e-17</td> <td> 3.16e-17</td> <td>   -2.227</td> <td> 0.026</td> <td>-1.33e-16</td> <td>-8.43e-18</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>241.943</td> <th>  Durbin-Watson:     </th> <td>   2.107</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 534.708</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.580</td>  <th>  Prob(JB):          </th> <td>7.75e-117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.908</td>  <th>  Cond. No.          </th> <td>1.34e+16</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.34e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:               GBA6RTBK   R-squared:                       0.470\n",
       "Model:                            OLS   Adj. R-squared:                  0.435\n",
       "Method:                 Least Squares   F-statistic:                     13.46\n",
       "Date:                Wed, 01 Dec 2021   Prob (F-statistic):          3.15e-232\n",
       "Time:                        21:22:55   Log-Likelihood:                -10624.\n",
       "No. Observations:                2574   AIC:                         2.157e+04\n",
       "Df Residuals:                    2414   BIC:                         2.250e+04\n",
       "Df Model:                         159                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1          1.606e-05   5.42e-06      2.965      0.003    5.44e-06    2.67e-05\n",
       "x2         -1.108e-06   1.15e-06     -0.962      0.336   -3.37e-06    1.15e-06\n",
       "x3         -8.431e-07   8.19e-07     -1.029      0.304   -2.45e-06    7.64e-07\n",
       "x4         -8.409e-07    8.2e-07     -1.026      0.305   -2.45e-06    7.67e-07\n",
       "x5         -2.736e-07   3.48e-07     -0.787      0.431   -9.55e-07    4.08e-07\n",
       "x6          8.763e-07   1.16e-06      0.757      0.449   -1.39e-06    3.15e-06\n",
       "x7         -2.407e-07   2.97e-07     -0.811      0.418   -8.23e-07    3.41e-07\n",
       "x8         -1.018e-07    1.8e-07     -0.566      0.571   -4.54e-07    2.51e-07\n",
       "x9         -4.348e-08    1.8e-07     -0.242      0.809   -3.96e-07    3.09e-07\n",
       "x10        -4.389e-08   1.51e-07     -0.290      0.772    -3.4e-07    2.53e-07\n",
       "x11         4.437e-07   2.75e-07      1.611      0.107   -9.65e-08    9.84e-07\n",
       "x12         4.327e-07   2.76e-07      1.568      0.117   -1.08e-07    9.74e-07\n",
       "x13        -5.119e-07   5.24e-07     -0.977      0.329   -1.54e-06    5.16e-07\n",
       "x14         3.115e-07   3.48e-07      0.896      0.370    -3.7e-07    9.93e-07\n",
       "x15         2.364e-07   3.02e-07      0.784      0.433   -3.55e-07    8.28e-07\n",
       "x16        -8.914e-07   4.29e-07     -2.077      0.038   -1.73e-06   -4.99e-08\n",
       "x17        -8.564e-07   4.29e-07     -1.995      0.046    -1.7e-06   -1.47e-08\n",
       "x18        -9.138e-07   4.29e-07     -2.129      0.033   -1.76e-06   -7.23e-08\n",
       "x19        -8.889e-07   4.29e-07     -2.071      0.039   -1.73e-06   -4.71e-08\n",
       "x20        -3.701e-06   1.56e-06     -2.378      0.017   -6.75e-06   -6.49e-07\n",
       "x21        -4.244e-07   3.74e-07     -1.135      0.257   -1.16e-06    3.09e-07\n",
       "x22         -3.97e-07   3.74e-07     -1.062      0.288   -1.13e-06    3.36e-07\n",
       "x23        -4.037e-07   3.74e-07     -1.080      0.280   -1.14e-06    3.29e-07\n",
       "x24        -4.087e-07   3.74e-07     -1.093      0.275   -1.14e-06    3.25e-07\n",
       "x25        -3.971e-07   3.73e-07     -1.063      0.288   -1.13e-06    3.35e-07\n",
       "x26         4.133e-07   3.73e-07      1.107      0.268   -3.19e-07    1.15e-06\n",
       "x27        -2.599e-08   8.59e-09     -3.026      0.003   -4.28e-08   -9.15e-09\n",
       "x28        -7.117e-08   7.18e-08     -0.992      0.321   -2.12e-07    6.95e-08\n",
       "x29          1.48e-08   1.58e-08      0.934      0.350   -1.63e-08    4.59e-08\n",
       "x30         3.681e-06   1.19e-06      3.095      0.002    1.35e-06    6.01e-06\n",
       "x31        -3.678e-06   1.19e-06     -3.094      0.002   -6.01e-06   -1.35e-06\n",
       "x32         6.156e-06   2.28e-06      2.695      0.007    1.68e-06    1.06e-05\n",
       "x33         9.836e-06   3.38e-06      2.914      0.004    3.22e-06    1.65e-05\n",
       "x34         9.837e-06   3.38e-06      2.915      0.004    3.22e-06    1.65e-05\n",
       "x35        -5.016e-06   1.62e-06     -3.092      0.002    -8.2e-06   -1.83e-06\n",
       "x36          -4.9e-07   1.57e-07     -3.121      0.002   -7.98e-07   -1.82e-07\n",
       "x37        -4.468e-07    1.6e-07     -2.795      0.005    -7.6e-07   -1.33e-07\n",
       "x38        -4.349e-07   1.64e-07     -2.647      0.008   -7.57e-07   -1.13e-07\n",
       "x39        -2.791e-07   1.12e-07     -2.502      0.012   -4.98e-07   -6.04e-08\n",
       "x40        -9.391e-08   8.93e-08     -1.052      0.293   -2.69e-07    8.12e-08\n",
       "x41        -1.852e-07   6.79e-08     -2.729      0.006   -3.18e-07   -5.21e-08\n",
       "x42         -3.65e-07   1.59e-07     -2.291      0.022   -6.77e-07   -5.26e-08\n",
       "x43        -4.355e-07   1.59e-07     -2.742      0.006   -7.47e-07   -1.24e-07\n",
       "x44        -4.907e-07   1.57e-07     -3.120      0.002   -7.99e-07   -1.82e-07\n",
       "x45        -9.634e-07   2.11e-07     -4.577      0.000   -1.38e-06   -5.51e-07\n",
       "x46        -4.807e-07   1.58e-07     -3.046      0.002    -7.9e-07   -1.71e-07\n",
       "x47         3.013e-07   9.62e-08      3.132      0.002    1.13e-07     4.9e-07\n",
       "x48         1.104e-07   3.78e-08      2.919      0.004    3.62e-08    1.85e-07\n",
       "x49         7.469e-08   3.51e-08      2.125      0.034    5.77e-09    1.44e-07\n",
       "x50        -8.427e-08   1.43e-07     -0.590      0.555   -3.64e-07    1.96e-07\n",
       "x51         1.099e-07    1.4e-07      0.785      0.433   -1.65e-07    3.85e-07\n",
       "x52         5.557e-09   5.19e-08      0.107      0.915   -9.62e-08    1.07e-07\n",
       "x53        -4.295e-07   2.14e-07     -2.004      0.045    -8.5e-07   -9.33e-09\n",
       "x54        -3.066e-08   4.19e-08     -0.732      0.464   -1.13e-07    5.15e-08\n",
       "x55         3.403e-08   4.07e-08      0.836      0.403   -4.58e-08    1.14e-07\n",
       "x56         1.562e-09   3.47e-08      0.045      0.964   -6.65e-08    6.97e-08\n",
       "x57        -2.083e-07   5.88e-08     -3.544      0.000   -3.24e-07    -9.3e-08\n",
       "x58         9.299e-08   4.25e-08      2.190      0.029    9.74e-09    1.76e-07\n",
       "x59        -9.923e-08    3.8e-08     -2.608      0.009   -1.74e-07   -2.46e-08\n",
       "x60        -8.187e-08   4.24e-08     -1.930      0.054   -1.65e-07    1.31e-09\n",
       "x61         2.667e-07   1.06e-07      2.512      0.012    5.85e-08    4.75e-07\n",
       "x62        -8.967e-08   3.24e-08     -2.765      0.006   -1.53e-07   -2.61e-08\n",
       "x63        -4.028e-09    2.7e-08     -0.149      0.881   -5.69e-08    4.89e-08\n",
       "x64         8.895e-08   2.91e-08      3.058      0.002    3.19e-08    1.46e-07\n",
       "x65        -4.421e-07   1.26e-07     -3.518      0.000   -6.89e-07   -1.96e-07\n",
       "x66        -3.302e-07    4.1e-07     -0.805      0.421   -1.13e-06    4.75e-07\n",
       "x67        -6.563e-07   1.27e-07     -5.165      0.000   -9.06e-07   -4.07e-07\n",
       "x68         6.195e-08    4.2e-07      0.148      0.883   -7.62e-07    8.85e-07\n",
       "x69        -3.677e-07   1.33e-07     -2.764      0.006   -6.29e-07   -1.07e-07\n",
       "x70        -3.539e-07   4.23e-07     -0.836      0.403   -1.18e-06    4.76e-07\n",
       "x71        -3.925e-07   1.32e-07     -2.968      0.003   -6.52e-07   -1.33e-07\n",
       "x72        -4.628e-07   4.14e-07     -1.117      0.264   -1.28e-06     3.5e-07\n",
       "x73         -3.59e-07   1.51e-07     -2.382      0.017   -6.54e-07   -6.34e-08\n",
       "x74         -4.91e-07   4.65e-07     -1.056      0.291    -1.4e-06    4.21e-07\n",
       "x75        -3.976e-07   1.26e-07     -3.155      0.002   -6.45e-07    -1.5e-07\n",
       "x76        -3.293e-07   4.18e-07     -0.787      0.431   -1.15e-06    4.91e-07\n",
       "x77        -2.046e-07   5.27e-08     -3.882      0.000   -3.08e-07   -1.01e-07\n",
       "x78        -3.765e-07   1.25e-07     -3.009      0.003   -6.22e-07   -1.31e-07\n",
       "x79        -5.693e-07   4.13e-07     -1.379      0.168   -1.38e-06     2.4e-07\n",
       "x80        -4.779e-07   1.22e-07     -3.906      0.000   -7.18e-07   -2.38e-07\n",
       "x81        -3.459e-07   4.11e-07     -0.842      0.400   -1.15e-06    4.59e-07\n",
       "x82         1.093e-07   2.15e-07      0.508      0.612   -3.13e-07    5.32e-07\n",
       "x83        -9.932e-07   4.55e-07     -2.182      0.029   -1.89e-06   -1.01e-07\n",
       "x84        -4.411e-07   1.22e-07     -3.609      0.000   -6.81e-07   -2.01e-07\n",
       "x85        -6.546e-08   4.25e-07     -0.154      0.878   -8.99e-07    7.68e-07\n",
       "x86         4.224e-07   1.34e-07      3.145      0.002    1.59e-07    6.86e-07\n",
       "x87         3.157e-07   4.24e-07      0.744      0.457   -5.17e-07    1.15e-06\n",
       "x88         6.326e-09   3.15e-08      0.201      0.841   -5.54e-08     6.8e-08\n",
       "x89         7.028e-08   3.74e-08      1.877      0.061   -3.13e-09    1.44e-07\n",
       "x90         3.074e-07   6.74e-08      4.560      0.000    1.75e-07     4.4e-07\n",
       "x91        -1.163e-07   1.27e-07     -0.916      0.360   -3.66e-07    1.33e-07\n",
       "x92         2.084e-08   2.36e-08      0.883      0.377   -2.54e-08    6.71e-08\n",
       "x93         -2.39e-07   1.89e-07     -1.262      0.207    -6.1e-07    1.32e-07\n",
       "x94         -1.31e-08   2.62e-08     -0.500      0.617   -6.44e-08    3.82e-08\n",
       "x95        -6.722e-09   5.32e-09     -1.264      0.206   -1.71e-08     3.7e-09\n",
       "x96        -7.869e-09   2.09e-08     -0.376      0.707   -4.89e-08    3.31e-08\n",
       "x97         5.768e-08   2.52e-08      2.286      0.022    8.21e-09    1.07e-07\n",
       "x98        -2.257e-09   8.61e-08     -0.026      0.979   -1.71e-07    1.67e-07\n",
       "x99         2.287e-09   9.31e-09      0.246      0.806    -1.6e-08    2.06e-08\n",
       "x100       -3.718e-08   2.61e-08     -1.424      0.154   -8.84e-08     1.4e-08\n",
       "x101       -8.966e-09   5.84e-08     -0.154      0.878   -1.23e-07    1.06e-07\n",
       "x102       -2.238e-07   1.51e-07     -1.487      0.137   -5.19e-07    7.14e-08\n",
       "x103       -4.351e-07   1.38e-07     -3.158      0.002   -7.05e-07   -1.65e-07\n",
       "x104        6.598e-08   8.01e-08      0.824      0.410    -9.1e-08    2.23e-07\n",
       "x105        8.436e-07   3.26e-07      2.585      0.010    2.04e-07    1.48e-06\n",
       "x106        -1.29e-07   8.75e-08     -1.475      0.140   -3.01e-07    4.25e-08\n",
       "x107        -9.39e-08   7.22e-08     -1.301      0.193   -2.35e-07    4.77e-08\n",
       "x108        2.768e-08   4.86e-08      0.569      0.569   -6.76e-08    1.23e-07\n",
       "x109       -6.092e-08   4.01e-08     -1.517      0.129    -1.4e-07    1.78e-08\n",
       "x110        2.889e-07   5.72e-08      5.054      0.000    1.77e-07    4.01e-07\n",
       "x111         2.28e-07   3.71e-08      6.145      0.000    1.55e-07    3.01e-07\n",
       "x112       -2.003e-07    5.2e-08     -3.853      0.000   -3.02e-07   -9.84e-08\n",
       "x113       -2.118e-08   2.36e-08     -0.898      0.369   -6.74e-08    2.51e-08\n",
       "x114        2.424e-09   7.19e-09      0.337      0.736   -1.17e-08    1.65e-08\n",
       "x115       -3.444e-09   6.87e-09     -0.501      0.616   -1.69e-08       1e-08\n",
       "x116        6.191e-08    1.2e-07      0.516      0.606   -1.74e-07    2.97e-07\n",
       "x117        2.874e-09   1.61e-08      0.179      0.858   -2.86e-08    3.44e-08\n",
       "x118         5.59e-06      2e-06      2.798      0.005    1.67e-06    9.51e-06\n",
       "x119        9.111e-06   6.29e-06      1.448      0.148   -3.23e-06    2.15e-05\n",
       "x120       -1.381e-05      1e-05     -1.379      0.168   -3.34e-05    5.83e-06\n",
       "x121        4.841e-07   1.85e-06      0.262      0.793   -3.14e-06    4.11e-06\n",
       "x122       -4.586e-07   5.57e-07     -0.824      0.410   -1.55e-06    6.33e-07\n",
       "x123       -1.816e-06   2.14e-06     -0.849      0.396   -6.01e-06    2.38e-06\n",
       "x124        8.098e-07   2.05e-06      0.395      0.693   -3.21e-06    4.83e-06\n",
       "x125           0.0009      0.000      4.063      0.000       0.000       0.001\n",
       "x126       -6.116e-05      0.000     -0.337      0.736      -0.000       0.000\n",
       "x127          -0.0025      0.002     -1.388      0.165      -0.006       0.001\n",
       "x128          -0.0004      0.000     -1.891      0.059      -0.001    1.49e-05\n",
       "x129          -0.0003      0.000     -0.889      0.374      -0.001       0.000\n",
       "x130       -1.476e-05      0.001     -0.021      0.983      -0.001       0.001\n",
       "x131          -0.0001      0.000     -1.042      0.297      -0.000       0.000\n",
       "x132       -9.698e-09   1.74e-08     -0.556      0.578   -4.39e-08    2.45e-08\n",
       "x133       -1.356e-06   1.68e-06     -0.807      0.419   -4.65e-06    1.94e-06\n",
       "x134        8.303e-07   8.39e-07      0.990      0.322   -8.14e-07    2.47e-06\n",
       "x135       -5.044e-06   1.28e-06     -3.927      0.000   -7.56e-06   -2.53e-06\n",
       "x136         1.03e-06   1.53e-06      0.673      0.501   -1.97e-06    4.03e-06\n",
       "x137        2.139e-06   1.47e-06      1.459      0.145   -7.37e-07    5.02e-06\n",
       "x138        2.706e-06   1.28e-06      2.107      0.035    1.88e-07    5.22e-06\n",
       "x139       -2.141e-07   1.45e-06     -0.148      0.882   -3.05e-06    2.63e-06\n",
       "x140           0.0006      0.000      2.328      0.020    9.49e-05       0.001\n",
       "x141           0.0006      0.000      2.678      0.007       0.000       0.001\n",
       "x142          -0.0015      0.000     -3.389      0.001      -0.002      -0.001\n",
       "x143           0.0008      0.000      1.694      0.090      -0.000       0.002\n",
       "x144           0.0013      0.000      2.882      0.004       0.000       0.002\n",
       "x145           0.0009      0.000      2.491      0.013       0.000       0.002\n",
       "x146       -8.885e-06      0.000     -0.036      0.972      -0.000       0.000\n",
       "x147       -4.143e-06    2.2e-06     -1.885      0.060   -8.45e-06    1.66e-07\n",
       "x148       -5.068e-06   2.97e-06     -1.708      0.088   -1.09e-05    7.52e-07\n",
       "x149        6.594e-05   3.74e-05      1.765      0.078   -7.31e-06       0.000\n",
       "x150       -1.591e-05   8.74e-06     -1.820      0.069    -3.3e-05    1.23e-06\n",
       "x151       -5.655e-09   1.05e-07     -0.054      0.957   -2.12e-07    2.01e-07\n",
       "x152         3.09e-06   1.47e-06      2.102      0.036    2.07e-07    5.97e-06\n",
       "x153        2.081e-06   9.99e-07      2.083      0.037    1.22e-07    4.04e-06\n",
       "x154       -2.374e-06   1.25e-06     -1.898      0.058   -4.83e-06    7.86e-08\n",
       "x155         5.72e-06   6.68e-06      0.857      0.392   -7.37e-06    1.88e-05\n",
       "x156        1.543e-07   2.01e-07      0.768      0.442   -2.39e-07    5.48e-07\n",
       "x157        2.675e-06   1.43e-06      1.865      0.062   -1.38e-07    5.49e-06\n",
       "x158        2.419e-06   9.95e-07      2.431      0.015    4.67e-07    4.37e-06\n",
       "x159       -2.357e-06   1.24e-06     -1.898      0.058   -4.79e-06    7.77e-08\n",
       "x160        3.713e-06   6.74e-06      0.551      0.582   -9.51e-06    1.69e-05\n",
       "x161       -1.844e-07   3.65e-07     -0.505      0.614   -9.01e-07    5.32e-07\n",
       "x162         1.93e-06   1.67e-06      1.156      0.248   -1.34e-06     5.2e-06\n",
       "x163        2.339e-06   1.01e-06      2.309      0.021    3.52e-07    4.33e-06\n",
       "x164        -1.83e-06   1.56e-06     -1.172      0.241   -4.89e-06    1.23e-06\n",
       "x165        6.616e-06   7.32e-06      0.903      0.366   -7.75e-06     2.1e-05\n",
       "x166       -2.368e-07   2.28e-07     -1.037      0.300   -6.85e-07    2.11e-07\n",
       "x167        3.305e-06   1.49e-06      2.213      0.027    3.76e-07    6.23e-06\n",
       "x168        2.063e-06   9.99e-07      2.065      0.039    1.04e-07    4.02e-06\n",
       "x169       -1.741e-06   1.24e-06     -1.403      0.161   -4.17e-06    6.92e-07\n",
       "x170        4.959e-06   6.71e-06      0.740      0.460   -8.19e-06    1.81e-05\n",
       "x171       -6.481e-07   5.59e-07     -1.158      0.247   -1.75e-06    4.49e-07\n",
       "x172        4.501e-06   1.78e-06      2.530      0.011    1.01e-06    7.99e-06\n",
       "x173        2.232e-06   1.02e-06      2.189      0.029    2.33e-07    4.23e-06\n",
       "x174       -1.108e-06   1.45e-06     -0.764      0.445   -3.95e-06    1.74e-06\n",
       "x175        5.722e-06   6.82e-06      0.839      0.402   -7.66e-06    1.91e-05\n",
       "x176        1.605e-07   2.51e-07      0.640      0.522   -3.31e-07    6.52e-07\n",
       "x177        2.913e-06   1.55e-06      1.878      0.060   -1.28e-07    5.95e-06\n",
       "x178        2.103e-06      1e-06      2.098      0.036    1.38e-07    4.07e-06\n",
       "x179       -2.299e-06   1.26e-06     -1.822      0.069   -4.77e-06    1.75e-07\n",
       "x180        2.988e-06   6.78e-06      0.441      0.660   -1.03e-05    1.63e-05\n",
       "const               0          0        nan        nan           0           0\n",
       "x181       -2.395e-06   1.12e-06     -2.133      0.033    -4.6e-06   -1.93e-07\n",
       "x182       -3.653e-07   1.14e-06     -0.320      0.749   -2.61e-06    1.87e-06\n",
       "x183        1.083e-06   1.75e-06      0.620      0.536   -2.34e-06    4.51e-06\n",
       "x184        4.911e-07   1.33e-06      0.368      0.713   -2.12e-06    3.11e-06\n",
       "x185       -3.858e-06   2.02e-06     -1.913      0.056   -7.81e-06    9.62e-08\n",
       "x186        5.045e-06   5.86e-06      0.861      0.389   -6.45e-06    1.65e-05\n",
       "x187        2.265e-06   9.94e-07      2.279      0.023    3.16e-07    4.21e-06\n",
       "x188       -9.934e-08   2.35e-07     -0.422      0.673   -5.61e-07    3.62e-07\n",
       "x189        2.763e-06   1.38e-06      2.006      0.045    6.18e-08    5.46e-06\n",
       "x190        2.279e-06   9.96e-07      2.289      0.022    3.27e-07    4.23e-06\n",
       "x191       -1.863e-06   1.22e-06     -1.528      0.127   -4.25e-06    5.28e-07\n",
       "x192        5.228e-06   6.63e-06      0.789      0.430   -7.77e-06    1.82e-05\n",
       "x193       -2.927e-08    9.2e-08     -0.318      0.750    -2.1e-07    1.51e-07\n",
       "x194        2.986e-06   1.53e-06      1.951      0.051    -1.5e-08    5.99e-06\n",
       "x195        2.203e-06   9.97e-07      2.209      0.027    2.48e-07    4.16e-06\n",
       "x196       -1.924e-06   1.26e-06     -1.524      0.128    -4.4e-06    5.52e-07\n",
       "x197        6.044e-06    6.8e-06      0.888      0.374    -7.3e-06    1.94e-05\n",
       "x198       -4.593e-07    4.7e-07     -0.978      0.328   -1.38e-06    4.62e-07\n",
       "x199       -2.534e-05   1.52e-05     -1.668      0.095   -5.51e-05    4.45e-06\n",
       "x200        4.171e-06   1.25e-06      3.331      0.001    1.72e-06    6.63e-06\n",
       "x201        2.411e-05   1.26e-05      1.906      0.057   -6.91e-07    4.89e-05\n",
       "x202       -5.585e-05   7.25e-05     -0.771      0.441      -0.000    8.63e-05\n",
       "x203       -6.458e-07   8.84e-07     -0.730      0.465   -2.38e-06    1.09e-06\n",
       "x204        7.682e-07   3.12e-06      0.246      0.806   -5.35e-06    6.89e-06\n",
       "x205        2.141e-06   9.95e-07      2.152      0.032     1.9e-07    4.09e-06\n",
       "x206       -2.998e-06   1.77e-06     -1.690      0.091   -6.48e-06    4.82e-07\n",
       "x207        5.356e-06   6.98e-06      0.767      0.443   -8.33e-06     1.9e-05\n",
       "x208       -2.657e-06   1.39e-06     -1.916      0.055   -5.38e-06    6.17e-08\n",
       "x209         -2.2e-06   9.94e-07     -2.214      0.027   -4.15e-06   -2.51e-07\n",
       "x210                0          0        nan        nan           0           0\n",
       "x211       -5.215e-06   6.66e-06     -0.783      0.434   -1.83e-05    7.84e-06\n",
       "x212        1.118e-16      2e-16      0.559      0.576   -2.81e-16    5.04e-16\n",
       "x213       -7.049e-17   3.16e-17     -2.227      0.026   -1.33e-16   -8.43e-18\n",
       "==============================================================================\n",
       "Omnibus:                      241.943   Durbin-Watson:                   2.107\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              534.708\n",
       "Skew:                           0.580   Prob(JB):                    7.75e-117\n",
       "Kurtosis:                       4.908   Cond. No.                     1.34e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.34e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_public = sm.OLS(y_train_public_continuous, X_train_public_continuous).fit()\n",
    "model_public.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions\n",
    "Accuracy \n",
    "Check assumption: homoscedasticity of residuals, normality of residualts, no multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ''",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-403-b737b90d7077>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlog_odds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Plot log odds versus continuous variable to check for linearity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_private\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_odds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Logistic Regression Assumption Test\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2900\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2902\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2903\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "#Grab probabilities and calculate log odds\n",
    "pred = logreg_private.predict_proba(X_train_private)[:, 0]\n",
    "log_odds = np.log(pred / (1 - pred))\n",
    "#Plot log odds versus continuous variable to check for linearity\n",
    "plt.scatter(x = X_train_private[''].values, y = log_odds)\n",
    "plt.title(\"Logistic Regression Assumption Test\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ticklabel_format(axis='x', style='sci', scilimits=(0,0))\n",
    "plt.ylabel(\"Log-odds\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab probabilities and calculate log odds\n",
    "pred = logreg_public.predict_proba(X_train_public)[:, 0]\n",
    "log_odds = np.log(pred / (1 - pred))\n",
    "#Plot log odds versus continuous variable to check for linearity\n",
    "plt.scatter(x = X_train_public[''].values, y = log_odds)\n",
    "plt.title(\"Logistic Regression Assumption Test\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ticklabel_format(axis='x', style='sci', scilimits=(0,0))\n",
    "plt.ylabel(\"Log-odds\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Private"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:0.6515555555555556\n",
      "Validation Accuracy:0.6113333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy:\" + str(logreg_private.score(X_train_private, y_train_private)))\n",
    "scores = np.mean(cross_val_score(logreg_private, X_train_private, y_train_private, cv=5))\n",
    "print(\"Validation Accuracy:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.624\n"
     ]
    }
   ],
   "source": [
    "#Generate precision score for test set\n",
    "test_pred = logreg_private.predict(X_test_private)\n",
    "print(\"Test Accuracy:\" + str(logreg_private.score(X_test_private, y_test_private)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:0.6923076923076923\n",
      "Validation Accuracy:0.6134501907748102\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy:\" + str(logreg_public.score(X_train_public, y_train_public)))\n",
    "scores = np.mean(cross_val_score(logreg_public, X_train_public, y_train_public, cv=5))\n",
    "print(\"Validation Accuracy:\" + str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:0.6095571095571095\n"
     ]
    }
   ],
   "source": [
    "#Generate precision score for test set\n",
    "test_pred = logreg_public.predict(X_test_public)\n",
    "print(\"Test Accuracy:\" + str(logreg_public.score(X_test_public, y_test_public)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "Look at loan default rates, only looking at GR exaggerates selection bias (E.g. Ivy's have the best GR). Schools with missions to help low-income students often have lower graduation rates. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f697dcecb1d05ac00f1a3e6eaa77fcadd261c808812655cf66e71c69fa75e0c6"
  },
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
